<!DOCTYPE html>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	
	<meta content="IE=edge" http-equiv="X-UA-Compatible">
	<meta content="width=840, initial-scale=1.0" name="viewport">
	<meta content="Lizi Liao" name="author">
	<meta content="Homepage" name="description">

	<title>Lizi Liao - SMU - SCIS</title>
	<link rel="shortcut icon" href="https://www.ic.gatech.edu/sites/all/themes/coc_sub_theme/favicon.ico" type="image/vnd.microsoft.icon">
	<!-- Bootstrap core CSS -->
	<link href="./files/bootstrap.min.css" rel="stylesheet">
	<!-- Bootstrap theme -->
	<link href="./files/bootstrap-theme.min.css" rel="stylesheet">
	<!-- Bootstrap icon -->
	<link href="./files/bootstrap.icon-large.min.css" rel="stylesheet">
	<!-- Custom styles for this template -->
	<link href="./files/theme.css" rel="stylesheet">
	<!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
	<link rel="stylesheet" href="./files/font-awesome.min.css">

	<!--[if lt IE 9]>
	  <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
	  <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
	<![endif]-->
<script src="chrome-extension://njgehaondchbmjmajphnhlojfnbfokng/js/contentScripts/dom.js"></script></head>

<body>
   <script type="text/javascript" async="" src="./files/ga.js"></script>
	<div class="container-narrow">
		<div class="title">

			<h4><strong>Lizi Liao</strong> &nbsp;&nbsp;&nbsp;&nbsp; 
			
			<img class="img-thumbnail-lizi" height="300" hspace="10" src="./files/lizi_profile_image.jpg" style="float:right"> 
			
			</h4>
			Assistant Professor<br>
			Lee Kong Chian Fellow<br>
			<a href="https://scis.smu.edu.sg/">School of Computing and Information Systems</a> <br>
			<a href="https://www.smu.edu.sg/">Fudan University</a><br>
			<span class="glyphicon glyphicon-envelope"></span>&nbsp; <tt>lzliao at smu dot edu dot sg</tt><br>
			<b>Office: </b>SCIS2-4056<br>
			<a href="https://scholar.google.com.sg/citations?user=W2b08EUAAAAJ&hl=en">[Google Scholar]</a>

		    <br><br>
		    I am a faculty member of the <a href="https://scis.smu.edu.sg/">School of Computing and Information Systems</a> at <a href="https://www.smu.edu.sg/">SMU</a>. My research explores two questions: What are the underlying principles of humans understanding conversation context as well as making proper responses, and how we can implement them on machine learning models? Research on this topic has to necessarily be at the intersection of Machine Learning, Natural Language Processing and Multimedia. My group is specifically interested in task-oriented dialogues, proactive conversational agents, and multimodal conversational search and recommendation as the application target. I received my Ph.D. from NUS, advised by Professor <a href="https://scholar.google.com.sg/citations?user=Z9DWCBEAAAAJ&hl=zh-CN">Tat-Seng Chua</a>.
		    
		    <br>
			<br>
						
			<img src="./files/recruitment.png" width="20" alt="" style="border-style: none" align="center"> &nbsp; I'm recruiting 0-1 new PhD student every intake batch (apply to <a href="https://scis.smu.edu.sg/phd/online-application">PhD</a> program and list me as a potential advisor). Our group also has multiple positions for summer interns and visiting research students. Please feel free to email me with your CV if you are interested. <br> 
			

		</div>

<div class="navbar navbar-default" role="navigation">
		
   		<div class="container">
        
				<div class="navbar-collapse">
				<ul class="nav navbar-nav">
					<li>
						<a href="https://liziliao.github.io/#research">Research</a>
					</li>

					<li>
						<a href="https://liziliao.github.io/#team">Team</a>
					</li>


					<li>
						<a href="https://liziliao.github.io/#publications">Publications</a>
					</li>


					<li>
						<a href="https://liziliao.github.io/#teach">Teaching</a>
					</li>


					<li>
						<a href="https://liziliao.github.io/#publications">Code &amp; Data</a>
					</li>
				</ul>

          
			</div>
		
		</div>
		</div>


		<div class="content">
			<h5 class="text-primary" id="news">What's New</h5>
		         
			<!-- <ul class="news"> -->
			<ul>
				<li style="list-style: none"><br>
				</li>

			<!-- <ul class="news"> -->
			<li><b style="color: green; background-color: #ffff42">NEW</b> [Sep 2024], six papers accepted to EMNLP 2024, to appear</li>
			<li><b style="color: green; background-color: #ffff42">NEW</b> [May 2024], four papers accepted to ACL 2024: (1) <a href="https://liziliao.github.io/papers/ACL24_ChatPal.pdf">emotional support conversation</a>; (2) <a href="https://liziliao.github.io/papers/ACL24_planlikehuman.pdf">conversation planning</a>; (3) <a href="https://liziliao.github.io/papers/ACL24_TCE.pdf">temporal long context understanding</a>; (4) <a href="https://liziliao.github.io/papers/ACL24_SynCID.pdf">conversational intent discovery</a>.</li>
			<li><b style="color: green; background-color: #ffff42">NEW</b> [Apr 2024], check out our new IJCAI 2024 survey on <a href="https://liziliao.github.io/papers/A_Survey_on_Neural_Question_Generation.pdf">neural question generation</a>.</li>
			<li><b style="color: green; background-color: #ffff42">NEW</b> [Mar 2024], two papers accepted to SIGIR 2024: (1) <a href="https://liziliao.github.io/papers/SIGIR24_HPC.pdf">human-centered proactive conversational agents</a>; (2) <a href="https://liziliao.github.io/papers/DCRS_SIGIR24.pdf">conversational recommendation</a>.</li>
			<li><b style="color: green; background-color: #ffff42">NEW</b> [Mar 2024], three papers accepted to NAACL 2024: (1) <a href="https://liziliao.github.io/papers/NAACL_24_ALUP__Camera_Ready.pdf">new intent discovery</a>; (2) <a href="https://liziliao.github.io/papers/NAACL_24_Mix_Initiative_CRS__Camera_Ready.pdf">mix-initiative response generation</a>; (3) <a href="https://liziliao.github.io/papers/NAACL_2024_SGSH_Camera_ready.pdf">knowledge base question generation</a>.</li>
			<li><b style="color: green; background-color: #ffff42">NEW</b> [Feb 2024], check out our new The Web Conference paper on <a href="https://arxiv.org/pdf/2401.14009.pdf">transformer
for dynamic graph modeling</a>.</li>
			<li><b style="color: green; background-color: #ffff42">NEW</b> [Jan 2024], received the <a href="https://research.google/outreach/featured-research-collaborations/"> Google South Asia & Southeast Asia research awards 2023</a>. </li>
			<!--<li><b style="color: green; background-color: #ffff42">NEW</b> [Dec 2023], two papers accepted to AAAI 2024: (1) <a href="./papers/AAAI24_Harnessing.pdf"> sentiment quadruple extraction in dialogues</a>; (2) <a href="./papers/AAAI24_Reverse.pdf">dialogue commonsense inference</a>. </li>
			<li><b style="color: green; background-color: #ffff42">NEW</b> [Nov 2023], invited to attend the <a href="https://www.dagstuhl.de/en/seminars/seminar-calendar/seminar-details/24352"> Dagstuhl Perspectives Workshop</a> on Conversational Agents: A Framework for Evaluation (CAFE). </li>
			<li><b style="color: green; background-color: #ffff42">NEW</b> [Oct 2023], 4 papers accepted to EMNLP 2023: (1) <a href="./papers/RTCP.pdf"> target-driven conversation promotion</a>; (2) <a href="./papers/proactive_prompt.pdf">proactive dialogue systems</a>; (3) ClusterPrompt - <a href="./papers/clusterprompt.pdf" >new intent discovery</a>; (4) <a href="./papers/emnlp23_survey.pdf"> survey on task-oriented dialogues</a>. </li>
		   <li><b style="color: green; background-color: #ffff42">NEW</b> [Sep 2023], invited to serve as Doctoral Consortium Chair in <a href="https://sigir-2024.github.io/">SIGIR 2024</a>. </li>
			<li><b style="color: green; background-color: #ffff42">NEW</b> [Aug 2023], check out our new ECML/PKDD 2023 paper on <a href="https://arxiv.org/abs/2307.00968">active learning</a>.</li>
			<li><b style="color: green; background-color: #ffff42">NEW</b> [Aug 2023], talk at FPT Software AI Residency â€“ AI Center.</li>
			<li><b style="color: green; background-color: #ffff42">NEW</b> [Aug 2023], invited to be a mentor at the <a href="https://recsys.acm.org/recsys23/"> RecSys 2023</a> Doctoral Symposium.</li>
			<li><b style="color: green; background-color: #ffff42">NEW</b> [Jul 2023], two papers accepted to ACM Multimedia 2023: (1) <a href="https://arxiv.org/pdf/2308.04502.pdf">conversational multimodal emotion recognition</a>; (2) <a href="./papers/ACM_MM_2023_Weakly_VMR.pdf">iterative video moment retrieval</a>.</li>
			<li><b style="color: green; background-color: #ffff42">NEW</b> [Jul 2023], check out our new ACL 2023 paper on <a href="https://aclanthology.org/2023.findings-acl.849.pdf">conversational aspect-based sentiment quadruple analysis</a>.</li>
			<li><b style="color: green; background-color: #ffff42">NEW</b> [May 2023], give tutorial at SIGIR 2023 with <a href="https://infosense.cs.georgetown.edu/grace/">Grace </a> and <a href="https://ischool.uw.edu/people/faculty/profile/chirags">Chirag</a> on <a href="https://dl.acm.org/doi/abs/10.1145/3539618.3594250">Proactive Conversational Agents in the Post-ChatGPT World</a>.</li>
			
			<li><b style="color: green; background-color: #ffff42">NEW</b> [Jun 2021], paper on <a href="https://liziliao.github.io/papers/Ref2022.pdf">multimodal conversational response generation</a> is accepted at ACM Multimedia 2022. </li>
			<li><b style="color: green; background-color: #ffff42">NEW</b> [Mar 2021], check out our SIGIR 2022 paper on <a href="https://liziliao.github.io/papers/Str2022.pdf">structured and natural responses co-generation</a>. </li>
			-->
			</ul>		
		</div>




		<div class="content">
			<h5 class="text-primary" id="research">Research Highlights</h5> 
			

			<div class="projects" style="margin-left: 1em; margin-right: 1em">
				<div class="boxed">
					<p><a href="./files/pic_proactive.png"><img border="1px" hspace="10" vspace="10" src="./files/pic_proactive.png" style="float: right;" width="330px"></a>
					</p>

<h5><strong>Proactive Conversational AI</strong>
					</h5>


<p> We recently published one of the earliest works on developing proactive dialogue systems in the era of LLMs <a href="./papers/proactive_prompt.pdf">[EMNLP'23a]</a>. To improve the proactiveness of conversational agents, we research on automatic ontology expansion <a href="./papers/clusterprompt.pdf">[EMNLP'23b,</a>  <a href="https://liziliao.github.io/papers/Semi_supervised.pdf">EMNLP'22a]</a>, target-driven conversational recommendation <a href="./papers/RTCP.pdf">[EMNLP'23c]</a> and building unified user simulators for better support <a href="https://liziliao.github.io/papers/User_Simulation.pdf">[EMNLP'22b]</a>. We also actively organize tutorials about proactive conversational agents <a href="https://liziliao.github.io/papers/WSDM2023_Tutorial.pdf">[WSDM'23,</a> <a href="https://dl.acm.org/doi/abs/10.1145/3539618.3594250">SIGIR'23]</a> to discuss important issues in conversational responsesâ€™ quality control, including safety, appropriateness, language detoxication, hallucination, and alignment.</p>


				</div>
				<!-- end boxed-->

				<div class="boxed">
					<p><a href="./files/pic_mcsr.png"><img border="1px" hspace="10" src="./files/pic_mcsr.png" style="float: left;" vspace="10" width="330px"></a>
					</p>

<h5 style="text-align: right"><strong>Multimodal Conversational Search and Recommendation</strong>
					</h5>


<p> Search and recommendation systems prevail and have profound impact. We aim to bridge the information asymmetry problem between the user and system via multimodal conversation <a href="http://staff.ustc.edu.cn/~hexn/papers/mm18-multimodal-dialog.pdf">[MM'18]</a>. It centers on broader types of â€˜understandâ€™ the user and â€˜respondâ€™ to the user under certain context. Specifically, we look into multimodal dialogue understanding <a href="https://arxiv.org/abs/2308.04502">[MM'23a]</a>, state tracking <a href="./papers/TMM_State.pdf">[TMM'22]</a>, knowledge-aware response generation <a href="./papers/Str2022.pdf">[SIGIR'22a]</a> and response strategy modeling <a href="https://liziliao.github.io/papers/Ref2022.pdf">[MM'22]</a>. </p>					
					
				</div>
				<!-- end boxed-->

				<div class="boxed">
					<p><a href="./files/pic_applications.png"><img border="1px" hspace="10" src="./files/pic_applications.png" style="float: right;" vspace="5" width="380px"></a>
					</p>

<h5><strong>Conversation AI + X (ChatPal, Learning Companion) Interdisciplinary Research</strong>
					</h5>


					<p> We work on a range of interesting and useful applications that aims to improve human life and society with conversational AI. A line of our research has focused on utilizing LLMs as "teachers" to enhance smaller models in various tasks such as emotional support <a href="https://arxiv.org/pdf/2308.11584.pdf">[arXiv'23, </a> <a href="https://arxiv.org/abs/2308.04502">MM'23a]</a>. We also recently started to develop question generation models for online learning companion. We also take a great interest in multimodal data, including work on human-in-the-loop video monent retrieval<a href="./papers/ACM_MM_2023_Weakly_VMR.pdf">[MMâ€™23b]</a> and e-commerce data towards intelligent shopping assistant <a href="https://liziliao.github.io/papers/Learning_to_Ask.pdf">[SIGIRâ€™22b]</a>. </p>



				</div>
				<!-- end boxed-->
			</div>
		</div>

		 
		<div class="content" id="team">
			
			<h5 class="text-primary">CoAgent Lab</h5>
			
			<!--<div class="projects" style="margin-left: 1em; margin-right: 1em">
				<div class="boxed">			
					<p><a href="./files/activities1.png"><img border="1px" hspace="5" vspace="4" src="./files/activities1.png" style="float: left;" width="400px"></a>
					</p>
					<p><a href="./files/activities2.png"><img border="1px" hspace="5" vspace="1" src="./files/activities2.png" style="float: right;" width="400px"></a>
					</p>
					<p><a href="./files/activities3.png"><img border="1px" hspace="5" vspace="4" src="./files/activities3.png" style="float: left;" width="400px"></a>
					</p>		

					<br>			
													
			</div></div>-->
			&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://liziliao.github.io/">Jinggui Liang</a> (PhD student) <br>
			&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://liziliao.github.io/">Huy Dao</a> (PhD student, SMU Presidential Doctoral Fellowship) <br>
			&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://liziliao.github.io/">Zhihan Zhang</a> (PhD student) <br>
			&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://liziliao.github.io/">Shuai Lin</a> (PhD student from HIT, co-supervise) <br>
			&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://liziliao.github.io/">Dung Vo </a> (Research Engineer) <br>
			&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://liziliao.github.io/">Yuqi Chu </a> (Visiting Research student from Hefei University of Technology) <br>
			&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://liziliao.github.io/">Tao He </a> (Visiting Research student from Harbin Institute of Technology) <br>
			&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://liziliao.github.io/">Xiaofeng Zhou </a> (Visiting Research student from Beijing Institute of Technology) <br>
			&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://liziliao.github.io/">Kun Zhu </a> (Visiting Research student from Harbin Institute of Technology) <br>
			<p>
			</p>
	 </div>
				    
				    
		<div class="content">
			<h5 class="text-primary" id="publications">Publications</h5>

			<ul>
				<li> MSMDFusion: Fusing LiDAR and Camera at Multiple Scales with Multi-Depth Seeds for 3D Object Detection <a href="https://arxiv.org/abs/2209.03102">pdf</a><br> Yang Jiao, Zequn Jie, Shaoxiang Chen, <b>Jingjing Chen</b>, Lin Ma, Yu-Gang Jiang <br> CVPR, 2023.
				<br><br>
				</li>

				<li> Enhancing the Self-Universality for Transferable Targeted Attacks <a href="https://arxiv.org/abs/2209.03716">pdf</a><br> Zhipeng Wei, <b>Jingjing Chen</b>, Zuxuan Wu, Yu-Gang Jiang <br> CVPR, 2023.
				<br><br>
				</li>

				<li> SVFormer: Semi-Supervised Video Transformer for Action Recognition <a href="https://arxiv.org/abs/2211.13222">pdf</a><br> Zhen Xing, Qi Dai, Han Hu, <b>Jingjing Chen</b>, Zuxuan Wu, Yu-Gang Jiang <br> CVPR, 2023.
				<br><br>
				</li>

				<li> Cross-Modal Transferable Adversarial Attacks from Images to Videos <a href="https://arxiv.org/abs/2112.05379v1">pdf</a><br> Zhipeng Wei, <b>Jingjing Chen</b>, Zuxuan Wu, Yu-Gang Jiang <br> CVPR, 2022.
				<br><br>
				</li>

				<li> Balanced Contrastive Learning for Long-Tailed Visual Recognition <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhu_Balanced_Contrastive_Learning_for_Long-Tailed_Visual_Recognition_CVPR_2022_paper.html">pdf</a><br> Jianggang Zhu, Zheng Wang, <b>Jingjing Chen</b>, Yi-Ping Phoebe Chen, Yu-Gang Jiang <br> CVPR, 2022.
				<br><br>
				</li>

				<li> ObjectFormer for Image Manipulation Detection and Localization <a href="https://arxiv.org/abs/2203.14681">pdf</a><br> Junke Wang, Zuxuan Wu, <b>Jingjing Chen</b>, Xintong Han, Abhinav Shrivastava, Ser-Nam Lim, Yu-Gang Jiang <br> CVPR 2022.
				<br><br>
				</li>

				<li> MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes <a href="https://arxiv.org/abs/2203.05203">pdf</a><br> Yang Jiao, Shaoxiang Chen, Zequn Jie, <b>Jingjing Chen</b>, Lin Ma, Yu-Gang Jiang <br> ECCV 2022.
				<br><br>
				</li>

				<li> Generalized Meta-FDMixup: Cross-Domain Few-Shot Learning Guided by Labeled Target Data <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9942934">pdf</a><br> Yuqian Fu, Yanwei Fu, <b>Jingjing Chen</b>, Yu-Gang Jiang <br> IEEE TIP, 2022. 
				<br><br>
				</li>

				<li> TGDM: Target Guided Dynamic Mixup for Cross-Domain Few-Shot Learning <a href="http://arxiv.org/abs/2210.05392">pdf</a><br> Linhai Zhuo, Yuqian Fu, <b>Jingjing Chen</b>, Yixin Cao, Yu-Gang Jiang <br> ACM Multimedia, 2022.
				<br><br>
				</li>

				<li> ME-D2N: Multi-Expert Domain Decompositional Network for Cross-Domain Few-Shot Learning <a href="https://arxiv.org/abs/2210.05280">pdf</a><br> Yuqian Fu, Yu Xie, Yanwei Fu, <b>Jingjing Chen</b>, Yu-Gang Jiang<br> ACM Multimedia, 2022.
				<br><br>
				</li>

				<li> Mix-DANN and Dynamic-Modal-Distillation for Video Domain Adaptation <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548313">pdf</a><br> Yuehao Yin, Bin Zhu, <b>Jingjing Chen</b>, Lechao Cheng, Yu-Gang Jiang <br> ACM Multimedia, 2022.
				<br><br>
				</li>


			    <li> Boosting the Transferability of Video Adversarial Examples via Temporal Translation <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20168/19927">pdf</a><br> Zhipeng Wei, <b>Jingjing Chen*</b>, Zuxuan Wu, Yu-Gang Jiang*<br> AAAI, 2022.
				<br><br>
				</li>
				
				<li> Attacking Video Recognition Models with Bullet-Screen Comments <a href="https://ojs.aaai.org/index.php/AAAI/article/view/19907/19666">pdf</a><br> Kai Chen, Zhipeng Wei, <b>Jingjing Chen*</b>, Zuxuan Wu, Yu-Gang Jiang*<br> AAAI, 2022.
				<br><br>
				</li>
				
				<li> Towards transferable adversarial attacks on vision transformers <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20169/19928">pdf</a><br> Zhipeng Wei, <b>Jingjing Chen</b>, Micah Goldblum, Zuxuan Wu, Tom Goldstein, Yu-Gang Jiang<br> AAAI, 2022.
				<br><br>
				</li>
				
			    <li> Two-stage Visual Cues Enhancement Network for Referring Image Segmentation <a href="https://arxiv.org/pdf/2110.04435.pdf">pdf</a><br> Yang Jiao, Zequn Jie, Weixin Luo, <b>Jingjing Chen*</b>, Yu-Gang Jiang, Xiaolin Wei, Lin Ma*<br> ACM Multimedia, 2021.
				<br><br>
				</li>
				
				<li> Visual Co-Occurrence Alignment Learning for Weakly-Supervised Video Moment Retrieval <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475278">pdf</a><br> Zheng Wang, <b>Jingjing Chen</b>, Yu-Gang Jiang<br> ACM Multimedia, 2021.
				<br><br>
				</li>
				
				<li> Fine-grained Cross-modal Alignment Network for Text-Video Retrieval <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475241">pdf</a><br> Ning Han, <b>Jingjing Chen</b>, Guangyi Xiao, Hao Zhang, Yawen Zeng, Hao Chen<br> ACM Multimedia, 2021.
				<br><br>
				</li>
				
			    <li> VideoLT: Large-scale Long-tailed Video Recognition <a href="https://arxiv.org/abs/2105.02668">pdf</a><br> Xing Zhang, Zuxuan Wu, Zejia Weng, Huazhu Fu, <b>Jingjing Chen</b>, Yu-Gang Jiang, Larry Davis<br> ICCV, 2021.
				<br><br>
				</li>
				
			    <li> Spatial-temporal Graphs for Cross-modal Text2Video Retrieval <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413769">pdf</a><br> Xue Song, <b>Jingjing Chen</b>, Zuxuan Wu, Yu-Gang Jiang<br> IEEE TMM, 2021.
				<br><br>
				</li>
				
				<li> Mixed Dish Recognition with Contextual Relation and Domain Alignment <a href="https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=7411&amp;context=sis_research">pdf</a><br> Lixi Deng, <b>Jingjing Chen*</b>, Chong-Wah Ngo, Qianru Sun, Sheng Tang, Yongdong Zhang, Tat-Seng Chua<br> IEEE TMM, 2021.
				<br><br>
				</li>
				
				<li> A Study of Multi-Task and Region-Wise Deep Learning for Food Ingredient Recognition <a href="https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=7304&amp;context=sis_research">pdf</a><br> <b>Jingjing Chen</b>, Bin Zhu, Chong-Wah Ngo, Tat-Seng Chua, Yu-Gang Jiang<br> IEEE TIP, 2020.
				<br><br>
				</li>
				
			    <li> A Hybrid Approach for Detecting Prerequisite Relations in Multi-modal Food Recipes <a href="https://ieeexplore.ieee.org/document/9288707">pdf</a><br> Liangming Pan, <b>Jingjing Chen*</b>, Shaoteng Liu, Chong-Wah Ngo, Min-Yen Kan, Tat-Seng Chua<br> IEEE TMM, 2020.
				<br><br>
				</li>
				
			    <li> Wilddeepfake: A challenging real-world dataset for deepfake detection <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413769">pdf</a><br> Bojia Zi, Minghao Chang, <b>Jingjing Chen</b>, Xingjun Ma, Yu-Gang Jiang<br> ACM Multimedia, 2020.
				<br><br>
				</li>
				
			    <li> Multi-modal cooking workflow construction for food recipes <a href="https://arxiv.org/abs/2008.09151">pdf</a><br> Liang-Ming Pan, <b>Jingjing Chen*</b>, Jianlong Wu, Shaoteng Liu, Chong-Wah Ngo, Min-Yen Kan, Yugang Jiang, Tat-Seng Chua<br> IEEE TMM, 2020.
				<br><br>
				</li>
				
			    <li> Person-level Action Recognition in Complex Events via TSD-TSM Networks <a href="https://dl.acm.org/doi/10.1145/3394171.3416276">pdf</a><br> Yanbin Hao, Zi-Niu Liu, Hao Zhang, Bin Zhu, <b>Jingjing Chen</b>, Yu-Gang Jiang, Chong-Wah Ngo<br> ACM Multimedia, 2020.
				<br><br>
				</li>
				
				 <li> Video Relation Detection via Multiple Hypothesis Association <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413764">pdf</a><br> Zixuan Su, Xindi Shang, <b>Jingjing Chen</b>, Yu-Gang Jiang, Zhiyong Qiu, Tat-Seng Chua<br> ACM Multimedia, 2020.
				<br><br>
				</li>
				
			    <li> Cross-domain Cross-modal Food Transfer <a href="https://dl.acm.org/doi/10.1145/3394171.3413809">pdf</a><br> Bin Zhu, Chong-Wah Ngo, <b>Jingjing Chen</b><br> ACM Multimedia, 2020.
				<br><br>
				</li>
				
			    <li>Visual Relations Augmented Cross-modal Retrieval <a href="https://dl.acm.org/doi/10.1145/3372278.3390709">pdf</a><br> Yutian Guo, <b>Jingjing Chen</b>, Hao Zhang, Yu-Gang Jiang<br> ACM ICMR, 2020.
				<br><br>
				</li>
				
			    <li> Hyperbolic Visual Embedding Learning for Zero-Shot Recognition <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Hyperbolic_Visual_Embedding_Learning_for_Zero-Shot_Recognition_CVPR_2020_paper.pdf">pdf</a><br> Shaoteng Liu, <b>Jingjing Chen*</b>, Liangming Pan, Chong-Wah Ngo, Tat-Seng Chua<br> CVPR, 2020.
				<br><br>
				</li>
			    
			    <li> Zero-shot Ingredient Recognition by Multi-Relational Graph Convolutional Network <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6626">pdf</a><br><b>Jingjing Chen</b>, Liangming Pan, Zhipeng Wei, Xiang Wang, Chong-Wah Ngo, Tat-Seng Chua<br> AAAI, New York, USA, February, 2020.
				<br><br>
				</li>
				
				<li> Heuristic Black-box Adversarial Attacks on Video Recognition Models <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6918/6772">pdf</a><br>Zhipeng Wei, <b>Jingjing Chen</b>, Xingxing Wei, Linxi Jiang, Tat-Seng Chua, FengFeng Zhou, Yu-Gang Jiang<br> AAAI, New York, USA, February, 2020.
				<br><br>
				</li>
				
				<li> Mixed-dish Recognition with Contextual Relation Networks <a href="https://dl.acm.org/doi/10.1145/3343031.3351147">pdf</a> <br>Lixi Deng, <b>Jingjing Chen*</b>, Xiangnan He, Qianru Sun, Sheng Tang, Zhaoyan Ming, Yongdong Zhang and Tat-Seng Chua<br> ACM Multimedia (ACM MM), Nice, France, October, 2019. 
				<br><br>
				</li>
				
				<li> DietLens-Eout: Large Scale Restaurant Food Photo Recognition <a href="https://dl.acm.org/doi/10.1145/3323873.3326923">pdf</a><br>Zhipeng Wei, <b>Jingjing Chen</b>, Zhaoyan Ming, Chong-Wah NGO, Tat-Seng Chua and Fengfeng Zhou<br> ICMR, Ottawa, Canada, June, 2019. 
				<br><br>
				</li>
				
				<li> Mix-dish Recognition Through Multi-label Learning <a href="https://dl.acm.org/doi/10.1145/3326458.3326929">pdf</a><br>Yunan Wang, <b>Jingjing Chen</b>, Chong-Wah NGO, Zhaoyan Ming, Tat-Seng Chua and Wanli Zuo<br> Workshop on Multimedia for Cooking and Eating Activities (CEA), Ottawa, Canada, June, 2019.
				<br><br>
				</li>
				
				<li> R2GAN: Cross-modal Recipe Retrieval with Generative Adversarial Networks <a href="http://vireo.cs.cityu.edu.hk/papers/R2GAN.pdf">pdf</a><br>Bin Zhu,Chong-Wah NGO, <b>Jingjing Chen</b> and Yanbin Hao<br> CVPR, Long Beach, CA, USA, June, 2019.
				<br><br>
				</li>
				
				<li> Deep Understanding of Cooking Procedures for Cross-modal Recipe Retrieval <a href="http://vireo.cs.cityu.edu.hk/papers/2018_p1020-chen.pdf">pdf</a><br><b>Jingjing Chen</b>, Chong-Wah NGO, Fuli Feng and Tat-Seng Chua<br> ACM Multimedia (ACM MM), Seoul, Korea, October, 2018.
				<br><br>
				</li>
				
				<li> Cross-modal Recipe Retrieval with Stacked Attention Model <a href="http://vireo.cs.cityu.edu.hk/papers/2018_CrossmodalRecipe.pdf">pdf</a><br><b>Jingjing Chen</b>, Lei Pang and Chong-Wah NGO<br> Multimedia Tools and Applications, Volume 77, 2018.
				<br><br>
				</li>
				
				<li> Food Photo Recognition for Dietary Tracking: System and Experiment <a href="http://vireo.cs.cityu.edu.hk/papers/Dietary.pdf">pdf</a><br>Zhaoyan Ming, <b>Jingjing Chen</b>, Yu Cao, Ciaran Forde, Chong-Wah NGO and Tat-Seng Chua<br> Multimedia Modeling (MMM), Bangkok, Thailand, January, 2018.
				<br><br>
				</li>
				
				<li> Cross-modal Recipe Retrieval with Rich Food Attributes <a href="http://vireo.cs.cityu.edu.hk/papers/jingjingmm2017.pdf">pdf</a><br><b>Jingjing Chen</b>, Chong-Wah NGO and Tat-Seng Chua<br> ACM Multimedia (ACM MM), Mountain View, CA, USA, October, 2017. <span style="color:red;">(oral)</span>
				<br><br>
				</li>
				
				<li> PIC2DISH: A Customized Cooking Assistant System <a href="https://dl.acm.org/citation.cfm?doid=3123266.3126490">pdf</a><br>Yongsheng An, Yu Cao, <b>Jingjing Chen</b>, Chong-Wah Ngo, Jia Jia, Huanbo Luan and Tat-Seng Chua<br> ACM Multimedia (ACM MM), Mountain View, CA, USA, October, 2017.
				<br><br>
				</li>
				
				<li> Cross-modal Recipe Retrieval: How to Cook This Dish? <a href="http://vireo.cs.cityu.edu.hk/jingjing/papers/chen2017cross.pdf">pdf</a><br><b>Jingjing Chen</b>, Lei Pang and Chong-Wah NGO<br> International Conference on Multimedia Modeling(MMM), Reykjavi, Iceland, January, 2017. <span style="color:red;">(Best Student Paper Award)</span>
				<br><br>
				</li>
				
				<li> Deep-based Ingredient Recognition for Cooking Recipe Retrieval <a href="http://vireo.cs.cityu.edu.hk/jingjing/papers/chen2016deep.pdf">pdf</a><br><b>Jingjing Chen</b> and Chong-Wah Ngo<br> ACM Multimedia (ACM MM), Amsterdam, Netherlands, October, 2016. <span style="color:red;">(Best Student Paper Award)</span><br><br>
				</li>

				<li> Image aesthetics enhancement using composition-based saliency detection <br>Handong Zhao, <b>Jingjing Chen</b>, Yahong Han and Xiaochun Cao<br> Multimedia System, vol. 21, no. 2, pp. 159-168, 2015
				<br><br>
				</li>

				<li>VIREO@TRECVID 2014: Instance Search and Semantic Indexing <a href="http://vireo.cs.cityu.edu.hk/papers/VIREO_TRECVID2014_ins_sin.pdf">pdf</a><br> Wei Zhang, Hao Zhang, Ting Yao, Yijie Lu, <b>Jingjing Chen</b> and Chong-Wah NGO<br>NIST TRECVID Workshop (TRECVID'14), Orlando, USA, November, 2014
				<br><br>
				</li>
				
				<li>Object Coding on the Semantic Graph for Scene Classification <br><b>Jingjing Chen</b>, Yahong Han, Xiaochun Cao and Qi Tian<br> ACM Multimedia (ACM MM), Barcelona, Spain, October 2013.
				<br><br>
				</li>

				<li>Visual Saliency Detection Based on Photographic Composition <br><b>Jingjing Chen</b>, Handong Zhao, Yahong Han and Xiaochun Cao<br>International Conference on Internet Multimedia Computing and Service (ICIMCS), huangshan, Anhui, China, August 2013. <span style="color:red;">(Best Paper Honorable Mention)</span>
				<br><br>
				</li>
				
				
				<li> Feature selection with Spatial Path Coding for Multimedia Analysis <br>Yahong Han, <b>Jingjing Chen</b> and Xiaochun Cao<br>Information Sciences, vol. 281, pp. 523-525, 2014
				<br><br>
				</li>
				
				<li>Object clique representation for scene classification <br><b>Jingjing Chen</b>, Xiaochun Cao and Bao Zhang<br>International Conference on Pattern Recognition (ICPR), Tsukuba, Japan, November, 2012.
				<br><br>
				</li>
				
				
			</ul>
		</div>

		<div class="content" id="teach">
			<h5 class="text-primary">Teaching</h5>
			
			Current Offering:<br>
			<ul>
			<li><a href="https://liziliao.github.io/">Visual Analytics for Business Intelligence (IS428)</a> (undergraduate level - Autumn 2024)</li>
			<li><a href="https://liziliao.github.io/">Text Analytics and Application (ISSS609)</a> (graduate level - Autumn 2024)</li>
			 

			</ul>
			Previous Offerings:<br>
			<ul>
			<li><a href="https://liziliao.github.io/">Data Management (IS112)</a> (undergraduate level - Spring 2023)</li>	
			<li><a href="https://liziliao.github.io/">Visual Analytics for Business Intelligence (IS428)</a> (undergraduate level - Spring 2022, Spring 2023, Spring 2024)</li>
			<li><a href="https://liziliao.github.io/">IS/SMT Project Experience (Applications) (IS483)</a> (undergraduate level - Spring 2022)</li>
			</ul>

			<p>
			</p>
		</div>		

		<div class="content">
			<h5 class="text-primary">Service</h5>

				Organizing Committee:<br>
				 <ul>
				 <li><a href="https://sigir-2024.github.io/">ACL, EMNLP, NAACL, COLING 2024</a>, Area Chair</li>
				 <li><a href="https://sigir-2024.github.io/">SIGIR 2024</a>, Doctoral Consortium Chair</li>
				 <li><a href="https://www2024.thewebconf.org/">The Web Conference 2024 (known as WWW)</a>, Local Arrangements Chair</li>
				 <li><a href="https://www.wsdm-conference.org/2023/">WSDM 2023</a>, Finance and Registration Chair</li>
				 <li><a href="http://tcci.ccf.org.cn/conference/2023/">NLPCC 2022, 2023</a>, Area Chair</li>
				 <li><a href="https://2019.acmmm.org/index.html">ACM Multimedia 2019</a>, Program design</li> 
				</ul>

				Senior Programe Committee Member: 
				<ul>
				 <li>International Joint Conference on Artificial Intelligence (IJCAI 2021)</li> 
				</ul>
		</div>
			
		<div class="content">
			<h5 class="text-primary">Miscellaneous</h5>


			<p>When I have spare time, I enjoy reading books, hiking, and dancing. </p>

			<p>
			</p>
		</div>
		Webpage template borrowed from Prof. <a href="https://cocoxu.github.io/#advise">Wei Xu</a>.

	
</body></html>
