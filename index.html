
<!-- saved from url=(0026)https://liziliao.github.io/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Lizi Liao</title>
<link rel="stylesheet" type="text/css" href="./files/main.css">
</head>


<body>

<table>
<tbody><tr>
<td><img src="./files/llz.png" width="200"></td>
<td>
<div style="font-size:24; font-weight:bold">Lizi Liao</div>
<ul><div class="section">
Assistant Professor at<br>
<a href="https://scis.smu.edu.sg/"> School of Computing and Information Systems</a><br>
<a href="https://www.smu.edu.sg//"> Singapore Management University</a><br>
</div>
<div class="section">
<b>Email: </b> <tt>lzliao at smu dot edu dot sg</tt><br>
<b>Office: </b>SCIS2-4056<br>
</div></ul>
<div class="section">
<a href="https://liziliao.github.io/#publications">[Publications]</a>&nbsp;
<a href="https://liziliao.github.io/#teaching">[Teaching]</a>&nbsp;
<a href="https://liziliao.github.io/#services">[Services]</a>&nbsp;
<br><br>
<a href="https://scholar.google.com.sg/citations?user=W2b08EUAAAAJ&hl=en">[Google Scholar]</a>
<a href="https://github.com/liziliao">[GitHub]</a>
</div>
</td>
</tr>
</tbody></table>

<script type="text/javascript">
function hideshow(which){
if (!document.getElementById)
return
if (which.style.display=="block")
which.style.display="none"
else
which.style.display="block"
}
</script>


<h3>About</h3>
<div class="section">
<ul>
My research explores two questions: What are the underlying principles of humans understanding conversation context as well as making proper responses, and how we can implement them on machine learning models? Research on this topic has to necessarily be at the intersection of Machine Learning, Natural Language Processing and Multimedia. In my lab, we are specifically interested in task-oriented dialogues, proactive conversational agents, and multimodal conversational search and recomendation as the application target.<br><br>

I received my Ph.D. from National University of Singapore, advised by Professor <a href="https://scholar.google.com.sg/citations?user=Z9DWCBEAAAAJ&hl=zh-CN">Tat-Seng Chua</a>.</ul>
</div>


<h3>News</h3>
<div class="section">
<ul> <li> <b style="color: green; background-color: #ffff42">NEW</b> [July, 2022] The undergraduate student Chenchen Ye co-supervised by me win the <a href="https://www.comp.nus.edu.sg/news/2022-ourp-ocp-2122/" target="_blank" rel="noopener"><b>Outstanding Undergraduate Researcher</b></a> prize in NUS!
</li><li> <b style="color: green; background-color: #ffff42">NEW</b> [July, 2022] I was invited to chair a session in SIGIR 2022.
</li><li> <b style="color: green; background-color: #ffff42">NEW</b> [June, 2022] Our RERG paper was accepted to ACM Multimedia 2022!
</li><li> [Mar, 2022] Our CoGen paper was accepted to SIGIR 2022!
</li></ul>
</div>


<h3>Joining Our <b style="color: brown">CoAgent</b> Group</h3>
<div class="section">
<ul>
<b style="color: green">I am looking for student researchers and engineers to join our group in SMU. Our group also has multiple positions for summer interns and visiting research students. Please feel free to send me an email with your CV if you are interested in doing research with me. </b> <br><br>
</ul></div>    

<h3>Students &amp; Interns</h3>
<ul><li>Suyu Liu (Singapore Managemnet University)</li></ul> 
<ul><li>Yuxia Wu (Xi'an Jiaotong University)</li></ul> 
<ul><li>Zixuan Li (National University of Singapore)</li></ul>
<ul><li>Chengyu Huang (National University of Singapore)</li></ul>
<ul><li>Zhonghua Zheng (Harbin Institute of Technology (Shenzhen)) </li></ul> 
<ul><li>Tianhao Dai (Wuhan University)</li></ul>
<ul><li>Qin Yang (University of Electronic Science and Technology of China)</li></ul>
<ul><li>Yuxiang Nie (Beijing Institute of Technology)</li></ul>

<h3>Alumni</h3>
<ul><li>Tongyao Zhu (National University of Singapore)</li></ul>
<ul><li>Chenchen Ye (National University of Singapore)</li></ul>
<ul><li>Zheng Zhang (Tsinghua University)</li></ul>

<a name="publications"></a>
<h3>Selected Publications </h3><b> Look for the full publication list? Please visit <a href="https://scholar.google.com.sg/citations?user=W2b08EUAAAAJ&hl=en" target="_blank">Google Scholar</a>.</b></br>
<!-- =======================================================================!-->
<div class="mainsection">
<ul>

<table width="100%">

<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./papers/Ref2022.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://liziliao.github.io/papers/Ref2022.pdf">Reflecting on Experiences for Response Generation</a></b><br><br>Chenchen Ye, Lizi Liao*, Suyu Liu and Tat-Seng Chua<br><br>ACM Multimedia, 2022 (Full)<br><br>
<a href="https://liziliao.github.io/papers/Ref2022.pdf">paper</a>
&nbsp;<a href="https://yecchen.github.io/paper/RERG_mm22_slides.pdf">sides</a>
&nbsp;<a href="https://files.atypon.com/acm/a90ccbfa09565d39924a1fe4f73dc94f">video</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>


<tr><td><br></td></tr>
</tbody>

<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./papers/Str2022.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="./papers/Str2022.pdf">Structured and Natural Responses Co-generation for Conversational Search</a></b><br><br>Chenchen Ye, Lizi Liao*, Fuli Feng, Wei Ji and Tat-Seng Chua<br><br>SIGIR, 2022 (Full) <br><br>
<a href="./papers/Str2022.pdf">paper</a>
&nbsp;<a href="https://yecchen.github.io/paper/Co-Gen_sigir22_slides.pdf">sides</a>
&nbsp;<a href="https://dl.acm.org/doi/abs/10.1145/3477495.3532063">video</a>
&nbsp;<a href="https://github.com/yecchen/Co-Gen">code & data</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>

<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./papers/Learning_to_Ask.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="./papers/Learning_to_Ask.pdf">Learning to Ask Critical Questions for Assisting Product Search</a></b><br><br>Zixuan Li, Lizi Liao and Tat-Seng Chua<br><br>SIGIR, 2022 (eCom) <br><br>
<a href="./papers/Learning_to_Ask.pdf">paper</a>
&nbsp;<a href="https://yecchen.github.io/paper/Co-Gen_sigir22_slides.pdf">sides</a>
&nbsp;<a href="https://dl.acm.org/doi/abs/10.1145/3477495.3532063">video</a>
&nbsp;<a href="https://github.com/yecchen/Co-Gen">code & data</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>


<tr><td><br></td></tr>
</tbody>

<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./papers/TMM_State.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="./papers/TMM_State.pdf">State Graph Reasoning for Multimodal Conversational Recommendation</a></b><br><br>Yuxia Wu, Lizi Liao*, Gangyi Zhang, Wenqiang Lei, Guoshuai Zhao, Xueming Qian, Tat-Seng Chua<br><br>TMM, 2022 <br><br>
<a href="./papers/TMM_State.pdf">paper</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>

<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./papers/2021sigir_mmconv.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="./papers/2021sigir_mmconv.pdf">MMConv: An Environment for Multimodal Conversational Search across Multiple Domains</a></b><br><br>Lizi Liao, Le Hong Long, Zheng Zhang, Minlie Huang, Tat-Seng Chua<br><br>SIGIR, 2021 (Full) <br><br>
<a href="./papers/2021sigir_mmconv.pdf">paper</a>
&nbsp;<a href="https://github.com/liziliao/MMConv">code & data</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>

<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./papers/2021www_dst.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="./papers/2021www_dst.pdf">Multi-domain Dialogue State Tracking with Recursive Inference</a></b><br><br>Lizi Liao, Tongyao Zhu, Le Hong Long, Tat-Seng Chua<br><br>WWW , 2021 (Full) <br><br>
<a href="./papers/2021www_dst.pdf">paper</a>
&nbsp;<a href="https://github.com/budzianowski/multiwoz/tree/master/data">data</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>

<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./papers/Enriching.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="./papers/Enriching.pdf">Towards Enriching Responses with Crowd-sourced Knowledge for Task-oriented Dialogue</a></b><br><br>Yingxu He, Lizi Liao, Zheng Zhang, Tat-Seng Chua<br><br>ACM MM, 2021 (workshop) <br><br>
<a href="./papers/Enriching.pdf">paper</a>
&nbsp;<a href="https://github.com/liziliao/MMConv">code & data</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./papers/DCR.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="./papers/DCR.pdf">Topic-Guided Relational Conversational Recommender in Multiple Domains</a></b><br><br>Lizi Liao, Ryuichi Takanobu, Yunshan Ma, Xun Yang, Minlie Huang, Tat-Seng Chua<br><br>TKDE, 2020 <br><br>
<a href="./papers/DCR.pdf">paper</a>
&nbsp;<a href="https://github.com/truthless11/DCR">code & data</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./papers/incremental.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="./papers/2020tacl_redst.pdf">Dialogue State Tracking with Incremental Reasoning</a></b><br><br>Lizi Liao, Le Hong Long, Yunshan Ma, Wenqiang Lei, Tat-Seng Chua<br><br>TACL, 2020 <br><br>
<a href="./papers/2020tacl_redst.pdf">paper</a>
&nbsp;<a href="https://github.com/budzianowski/multiwoz/tree/master/data">data</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>

<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./papers/NST.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="./papers/2019www_MultimediaDST_dialog.pdf">Neural Multimodal Belief Tracker with Adaptive Attention for Dialogue Systems</a></b><br><br>Zheng Zhang*, Lizi Liao*, Minlie Huang, Xiaoyan Zhu, Tat-Seng Chua<br><br>WWW, 2019 <br><br>
<a href="./papers/2019www_MultimediaDST_dialog.pdf">paper</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./papers/KMS.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="http://staff.ustc.edu.cn/~hexn/papers/mm18-multimodal-dialog.pdf">Knowledge-aware Multimodal Dialogue Systems</a> <font color="red">  (Best Paper Final List)</font> </b><br><br>Lizi Liao, Yunshan Ma, Xiangnan He, Richang Hong, Tat-Seng Chua<br><br>ACM Multimedia, 2018 <br><br>
<a href="http://staff.ustc.edu.cn/~hexn/papers/mm18-multimodal-dialog.pdf">paper</a>
&nbsp;<a href="https://amritasaha1812.github.io/MMD/">data</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>


<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./papers/interpretable.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://www.zhaobo.me/papers/mm_lizi.pdf">Interpretable Multimodal Retrieval for Fashion Products</a></b><br><br>Lizi Liao, Xiangnan He, Bo Zhao, Chong-Wah Ngo, Tat-Seng Chua<br><br>ACM Multimedia, 2018 <br><br>
<a href="https://www.zhaobo.me/papers/mm_lizi.pdf">paper</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>

<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./papers/attributed.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://arxiv.org/abs/1705.04969">Attributed Social Network Embedding</a> </b><br><br>Lizi Liao, Xiangnan He, Hanwang Zhang, Tat-Seng Chua<br><br>TKDE, 2018<br><br>
<a href="https://arxiv.org/abs/1705.04969">paper</a>
&nbsp;<a href="https://github.com/liziliao/ASNE">code & data</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>

<!--Concept Learning-->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./papers/neural.png" width="160" alt="" style="border-style: none" align="center"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="https://arxiv.org/abs/1708.05031">Neural Collaborative Filtering</a> </b><br><br>Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua<br><br>WWW, 2017<br><br>
<a href="https://arxiv.org/abs/1708.05031">paper</a>
&nbsp;<a href="https://github.com/liziliao/neural_collaborative_filtering">code & data</a> 
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
</tbody>

</table>
</ul>
</div>


<a name="teaching"></a>
<h3>Teaching Experiences</h3>
<div class="mainsection">
<ul>
<li>
Lecturer, Spring 2021-22: <a href="https://wiki.smu.edu.sg/1920t1is428g1/Main_Page">Visual Analytics for Business Intelligence (IS428)</a>
</li>
</ul>
<ul>
<li>
Lecturer, Spring 2021-22: <a href="https://x.smu.edu.sg/courses/project-experience-applications-1">IS/SMT Project Experience (Applications) (IS483)</a>
</li>
</ul>
</div>

<a name="services"></a>
<h3>Professional Activities</h3>
<div class="mainsection">
<ul>
<li>
Organizing Committee: 
<ul><li>ACM International Conference on Web Search and Data Mining (WSDM 2023), Finance and Registration Chair</li></ul>
<ul><li>International Conference on Natural Language Processing and Chinese Computing (NLPCC 2022), Area Chair</li></ul> 
<ul><li>ACM International Conference on Multimedia (ACM Multimedia 2019), Program design</li></ul> 
</li>
</ul>
<ul>
<li>
Senior Programe Committee Member: 
<ul><li>International Joint Conference on Artificial Intelligence (IJCAI 2021)</li></ul> 
</li>
</ul>
<ul>
<li>
Conference Reviewer: 
<ul><li>International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</li></ul> 
<ul><li>ACM International Conference on Multimedia (ACM Multimedia)</li></ul>  
<ul><li>Annual Meeting of the Association for Computational Linguistics (ACL)</li></ul> 
<ul><li>Conference on Empirical Methods in Natural Language Processing (EMNLP)</li></ul> 
<ul><li>International Conference on Learning Representations (ICLR)</li></ul> 
</li>
</ul>
<ul>
<li>
Journal Reviewer:
<ul><li>IEEE Transactions on Knowledge and Data Engineering (TKDE)</li></ul>
<ul><li>IEEE Transactions on Multimedia (TMM)</li></ul>    
<ul><li>Journal of Artificial Intelligence Research (JAIR)</li></ul>   
</li>
</ul>
</div>

<div></div>
</body></html>