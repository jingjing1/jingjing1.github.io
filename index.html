<!DOCTYPE html>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	
	<meta content="IE=edge" http-equiv="X-UA-Compatible">
	<meta content="width=840, initial-scale=1.0" name="viewport">
	<meta content="Lizi Liao" name="author">
	<meta content="Homepage" name="description">

	<title>Jingjing Chen - Fudan - SCIS</title>
	<link rel="shortcut icon" href="https://www.ic.gatech.edu/sites/all/themes/coc_sub_theme/favicon.ico" type="image/vnd.microsoft.icon">
	<!-- Bootstrap core CSS -->
	<link href="./files/bootstrap.min.css" rel="stylesheet">
	<!-- Bootstrap theme -->
	<link href="./files/bootstrap-theme.min.css" rel="stylesheet">
	<!-- Bootstrap icon -->
	<link href="./files/bootstrap.icon-large.min.css" rel="stylesheet">
	<!-- Custom styles for this template -->
	<link href="./files/theme.css" rel="stylesheet">
	<!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
	<link rel="stylesheet" href="./files/font-awesome.min.css">

	<!--[if lt IE 9]>
	  <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
	  <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
	<![endif]-->
<script src="chrome-extension://njgehaondchbmjmajphnhlojfnbfokng/js/contentScripts/dom.js"></script></head>

<body>
   <script type="text/javascript" async="" src="./files/ga.js"></script>
	<div class="container-narrow">
		<div class="title">

			<h4><strong>Jingjing Chen</strong> &nbsp;&nbsp;&nbsp;&nbsp; 
			
			<img class="img-thumbnail-lizi" height="300" hspace="10" src="./files/jingjingchen_profile_image.jpg" style="float:right"> 
			
			</h4>
			Assistant Professor<br>
			Lee Kong Chian Fellow<br>
			<a href="https://scis.smu.edu.sg/">Department of Computer Science</a> <br>
			<a href="https://www.smu.edu.sg/">Fudan University</a><br>
			<span class="glyphicon glyphicon-envelope"></span>&nbsp; <tt>jingjingchen at fudan dot edu dot sg</tt><br>
			<b>Office: </b>SCIS2-4056<br>
			<a href="https://scholar.google.com.hk/citations?user=DfWdqzQAAAAJ">[Google Scholar]</a>

		    <br><br>
		    <p>I am currently an Associate Professor at the School of Computer Science, Fudan University. Prior to joining Fudan, I was a Research Fellow at the School of Computing, <a href="https://www.nus.edu.sg/">National University of Singapore (NUS)</a>, working with <a href="https://www.chuatatseng.com/">Prof. Tat-Seng Chua</a>. I received my Ph.D. degree in Computer Science from City University of Hong Kong in July 2018, supervised by <a href="https://computing.smu.edu.sg/faculty/profile/601/ngo-chong-wah">Prof. Chong-Wah Ngo</a>. I received several accolades, including the Best Student Paper Award at ACM Multimedia 2016, the Best Student Paper Award at Multimedia Modeling 2017, the "<strong>ACM Shanghai Rising Star Award</strong>" in 2020, and the "<strong>AI 2000 Most Influential Scholar Nomination Award</strong>" in 2021, "<strong>ACM SIGMM Rising Star Award</strong>" in 2024, and listed as "<strong>Stanford/Elsevier’s Top 2% Scientist</strong>" in 2024, etc.</p>

		    <p>My research primarily focuses on, but is not limited to:</p>
			<ol>
			  <li><strong>Multimedia Content Analysis</strong>: Cross-modal Retrieval, Image/Video Classification, Multi-modal Forgery Detection, Food Recognition, and Nutrition Analysis</li>
			  <li><strong>Multimedia Model Security</strong>: Adversarial Attack and Defense, Transferable Adversarial Attack, Backdoor Attack, Adversarial Training, Adversarial Robustness Evaluation for Large Multi-modal Models, Unsafe Concept Removal for Generative Models</li>
			</ol>

		    <br>
			<br>
						
			<img src="./files/recruitment.png" width="20" alt="" style="border-style: none" align="center"> &nbsp; 
			<strong>Open Positions</strong>
			<be>
				<ul>
				  <li>I am looking for 2-3 interns working on Generative AI Security. Please feel free to email me with your CV if you are interested.</li>
				  <li>Currently, there are no openings for PhD students starting in Fall 2025.</li>
				</ul>
			<br> 
			

		</div>

<div class="navbar navbar-default" role="navigation">
		
   		<div class="container">
        
				<div class="navbar-collapse">
				<ul class="nav navbar-nav">
					<li>
						<a href="https://liziliao.github.io/#research">Research</a>
					</li>

					<li>
						<a href="https://liziliao.github.io/#team">Team</a>
					</li>


					<li>
						<a href="https://liziliao.github.io/#publications">Publications</a>
					</li>


					<li>
						<a href="https://liziliao.github.io/#teach">Teaching</a>
					</li>


					<li>
						<a href="https://liziliao.github.io/#publications">Code &amp; Data</a>
					</li>
				</ul>

          
			</div>
		
		</div>
		</div>


		<div class="content">
			<h5 class="text-primary" id="news">What's New</h5>
		         
			<!-- <ul class="news"> -->
			<ul>
				<li style="list-style: none"><br>
				</li>

			<!-- <ul class="news"> -->
			  <li><strong>[Nov 2024]</strong> My Ph.D. student, Weizhi Peng (graduated in June 2024), has been awarded the CSIG Outstanding Doctoral Dissertation Award! <a href="https://mp.weixin.qq.com/s/jjrRQ2tlBL_SBXrBmhqj0g">[link]</a></li>
			  <li><strong>[Nov 2024]</strong> I am appointed as an Associate Editor for ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)!</li>
			  <li><strong>[Oct 2024]</strong> I am awarded ACM SIGMM Rising Star Award! <a href="http://www.sigmm.org/news/sigmm_rising_star_award_2024">[link]</a></li>
			  <li><strong>[Oct 2024]</strong> One paper about open-set single-source domain generalization is accepted by TMM 2024!</li>
			  <li><strong>[Oct 2024]</strong> I am appointed as an Associate Editor for IEEE Transactions on Multimedia (TMM)!</li>
			  <li><strong>[Oct 2024]</strong> My master’s students, Ziyi Gao and Junhao Xv, have been awarded the National Scholarship!</li>
			  <li><strong>[Sep 2024]</strong> I am listed as “Stanford/Elsevier’s Top 2% Scientist in 2024!</li>
			  <li><strong>[July 2024]</strong> Five papers are accepted by ACM Multimedia 2024!</li>
			  <li><strong>[July 2024]</strong> Two papers are accepted by ECCV 2024!</li>
			  <li><strong>[May 2024]</strong> My PhD students Weizhi Peng and Tianwen Qian have successfully defended their doctoral dissertations!</li>
			  <li><strong>[May 2024]</strong> My master students Yanqi Wu, Yue Yu, Wenzhuo Xu, Yiqiang Lv, Xueqing Zhou have successfully defended their doctoral dissertations!</li>
			</ul>		
		</div>




		<div class="content">
			<h5 class="text-primary" id="research">Research Highlights</h5> 
			

			<div class="projects" style="margin-left: 1em; margin-right: 1em">
				<div class="boxed">
					<p><a href="./files/pic_proactive.png"><img border="1px" hspace="10" vspace="10" src="./files/pic_proactive.png" style="float: right;" width="330px"></a>
					</p>

<h5><strong>Proactive Conversational AI</strong>
					</h5>


<p> We recently published one of the earliest works on developing proactive dialogue systems in the era of LLMs <a href="./papers/proactive_prompt.pdf">[EMNLP'23a]</a>. To improve the proactiveness of conversational agents, we research on automatic ontology expansion <a href="./papers/clusterprompt.pdf">[EMNLP'23b,</a>  <a href="https://liziliao.github.io/papers/Semi_supervised.pdf">EMNLP'22a]</a>, target-driven conversational recommendation <a href="./papers/RTCP.pdf">[EMNLP'23c]</a> and building unified user simulators for better support <a href="https://liziliao.github.io/papers/User_Simulation.pdf">[EMNLP'22b]</a>. We also actively organize tutorials about proactive conversational agents <a href="https://liziliao.github.io/papers/WSDM2023_Tutorial.pdf">[WSDM'23,</a> <a href="https://dl.acm.org/doi/abs/10.1145/3539618.3594250">SIGIR'23]</a> to discuss important issues in conversational responses’ quality control, including safety, appropriateness, language detoxication, hallucination, and alignment.</p>


				</div>
				<!-- end boxed-->

				<div class="boxed">
					<p><a href="./files/pic_mcsr.png"><img border="1px" hspace="10" src="./files/pic_mcsr.png" style="float: left;" vspace="10" width="330px"></a>
					</p>

<h5 style="text-align: right"><strong>Multimodal Conversational Search and Recommendation</strong>
					</h5>


<p> Search and recommendation systems prevail and have profound impact. We aim to bridge the information asymmetry problem between the user and system via multimodal conversation <a href="http://staff.ustc.edu.cn/~hexn/papers/mm18-multimodal-dialog.pdf">[MM'18]</a>. It centers on broader types of ‘understand’ the user and ‘respond’ to the user under certain context. Specifically, we look into multimodal dialogue understanding <a href="https://arxiv.org/abs/2308.04502">[MM'23a]</a>, state tracking <a href="./papers/TMM_State.pdf">[TMM'22]</a>, knowledge-aware response generation <a href="./papers/Str2022.pdf">[SIGIR'22a]</a> and response strategy modeling <a href="https://liziliao.github.io/papers/Ref2022.pdf">[MM'22]</a>. </p>					
					
				</div>
				<!-- end boxed-->

				<div class="boxed">
					<p><a href="./files/pic_applications.png"><img border="1px" hspace="10" src="./files/pic_applications.png" style="float: right;" vspace="5" width="380px"></a>
					</p>

<h5><strong>Conversation AI + X (ChatPal, Learning Companion) Interdisciplinary Research</strong>
					</h5>


					<p> We work on a range of interesting and useful applications that aims to improve human life and society with conversational AI. A line of our research has focused on utilizing LLMs as "teachers" to enhance smaller models in various tasks such as emotional support <a href="https://arxiv.org/pdf/2308.11584.pdf">[arXiv'23, </a> <a href="https://arxiv.org/abs/2308.04502">MM'23a]</a>. We also recently started to develop question generation models for online learning companion. We also take a great interest in multimodal data, including work on human-in-the-loop video monent retrieval<a href="./papers/ACM_MM_2023_Weakly_VMR.pdf">[MM’23b]</a> and e-commerce data towards intelligent shopping assistant <a href="https://liziliao.github.io/papers/Learning_to_Ask.pdf">[SIGIR’22b]</a>. </p>



				</div>
				<!-- end boxed-->
			</div>
		</div>

		 
				    
				    
		<div class="content">
			<h5 class="text-primary" id="publications">Publications</h5>

			<ul>
				<li> MSMDFusion: Fusing LiDAR and Camera at Multiple Scales with Multi-Depth Seeds for 3D Object Detection <a href="https://arxiv.org/abs/2209.03102">pdf</a><br> Yang Jiao, Zequn Jie, Shaoxiang Chen, <b>Jingjing Chen</b>, Lin Ma, Yu-Gang Jiang <br> CVPR, 2023.
				<br><br>
				</li>

				<li> Enhancing the Self-Universality for Transferable Targeted Attacks <a href="https://arxiv.org/abs/2209.03716">pdf</a><br> Zhipeng Wei, <b>Jingjing Chen</b>, Zuxuan Wu, Yu-Gang Jiang <br> CVPR, 2023.
				<br><br>
				</li>

				<li> SVFormer: Semi-Supervised Video Transformer for Action Recognition <a href="https://arxiv.org/abs/2211.13222">pdf</a><br> Zhen Xing, Qi Dai, Han Hu, <b>Jingjing Chen</b>, Zuxuan Wu, Yu-Gang Jiang <br> CVPR, 2023.
				<br><br>
				</li>

				<li> Cross-Modal Transferable Adversarial Attacks from Images to Videos <a href="https://arxiv.org/abs/2112.05379v1">pdf</a><br> Zhipeng Wei, <b>Jingjing Chen</b>, Zuxuan Wu, Yu-Gang Jiang <br> CVPR, 2022.
				<br><br>
				</li>

				<li> Balanced Contrastive Learning for Long-Tailed Visual Recognition <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhu_Balanced_Contrastive_Learning_for_Long-Tailed_Visual_Recognition_CVPR_2022_paper.html">pdf</a><br> Jianggang Zhu, Zheng Wang, <b>Jingjing Chen</b>, Yi-Ping Phoebe Chen, Yu-Gang Jiang <br> CVPR, 2022.
				<br><br>
				</li>

				<li> ObjectFormer for Image Manipulation Detection and Localization <a href="https://arxiv.org/abs/2203.14681">pdf</a><br> Junke Wang, Zuxuan Wu, <b>Jingjing Chen</b>, Xintong Han, Abhinav Shrivastava, Ser-Nam Lim, Yu-Gang Jiang <br> CVPR 2022.
				<br><br>
				</li>

				<li> MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes <a href="https://arxiv.org/abs/2203.05203">pdf</a><br> Yang Jiao, Shaoxiang Chen, Zequn Jie, <b>Jingjing Chen</b>, Lin Ma, Yu-Gang Jiang <br> ECCV 2022.
				<br><br>
				</li>

				<li> Generalized Meta-FDMixup: Cross-Domain Few-Shot Learning Guided by Labeled Target Data <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9942934">pdf</a><br> Yuqian Fu, Yanwei Fu, <b>Jingjing Chen</b>, Yu-Gang Jiang <br> IEEE TIP, 2022. 
				<br><br>
				</li>

				<li> TGDM: Target Guided Dynamic Mixup for Cross-Domain Few-Shot Learning <a href="http://arxiv.org/abs/2210.05392">pdf</a><br> Linhai Zhuo, Yuqian Fu, <b>Jingjing Chen</b>, Yixin Cao, Yu-Gang Jiang <br> ACM Multimedia, 2022.
				<br><br>
				</li>

				<li> ME-D2N: Multi-Expert Domain Decompositional Network for Cross-Domain Few-Shot Learning <a href="https://arxiv.org/abs/2210.05280">pdf</a><br> Yuqian Fu, Yu Xie, Yanwei Fu, <b>Jingjing Chen</b>, Yu-Gang Jiang<br> ACM Multimedia, 2022.
				<br><br>
				</li>

				<li> Mix-DANN and Dynamic-Modal-Distillation for Video Domain Adaptation <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548313">pdf</a><br> Yuehao Yin, Bin Zhu, <b>Jingjing Chen</b>, Lechao Cheng, Yu-Gang Jiang <br> ACM Multimedia, 2022.
				<br><br>
				</li>


			    <li> Boosting the Transferability of Video Adversarial Examples via Temporal Translation <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20168/19927">pdf</a><br> Zhipeng Wei, <b>Jingjing Chen*</b>, Zuxuan Wu, Yu-Gang Jiang*<br> AAAI, 2022.
				<br><br>
				</li>
				
				<li> Attacking Video Recognition Models with Bullet-Screen Comments <a href="https://ojs.aaai.org/index.php/AAAI/article/view/19907/19666">pdf</a><br> Kai Chen, Zhipeng Wei, <b>Jingjing Chen*</b>, Zuxuan Wu, Yu-Gang Jiang*<br> AAAI, 2022.
				<br><br>
				</li>
				
				<li> Towards transferable adversarial attacks on vision transformers <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20169/19928">pdf</a><br> Zhipeng Wei, <b>Jingjing Chen</b>, Micah Goldblum, Zuxuan Wu, Tom Goldstein, Yu-Gang Jiang<br> AAAI, 2022.
				<br><br>
				</li>
				
			    <li> Two-stage Visual Cues Enhancement Network for Referring Image Segmentation <a href="https://arxiv.org/pdf/2110.04435.pdf">pdf</a><br> Yang Jiao, Zequn Jie, Weixin Luo, <b>Jingjing Chen*</b>, Yu-Gang Jiang, Xiaolin Wei, Lin Ma*<br> ACM Multimedia, 2021.
				<br><br>
				</li>
				
				<li> Visual Co-Occurrence Alignment Learning for Weakly-Supervised Video Moment Retrieval <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475278">pdf</a><br> Zheng Wang, <b>Jingjing Chen</b>, Yu-Gang Jiang<br> ACM Multimedia, 2021.
				<br><br>
				</li>
				
				<li> Fine-grained Cross-modal Alignment Network for Text-Video Retrieval <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475241">pdf</a><br> Ning Han, <b>Jingjing Chen</b>, Guangyi Xiao, Hao Zhang, Yawen Zeng, Hao Chen<br> ACM Multimedia, 2021.
				<br><br>
				</li>
				
			    <li> VideoLT: Large-scale Long-tailed Video Recognition <a href="https://arxiv.org/abs/2105.02668">pdf</a><br> Xing Zhang, Zuxuan Wu, Zejia Weng, Huazhu Fu, <b>Jingjing Chen</b>, Yu-Gang Jiang, Larry Davis<br> ICCV, 2021.
				<br><br>
				</li>
				
			    <li> Spatial-temporal Graphs for Cross-modal Text2Video Retrieval <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413769">pdf</a><br> Xue Song, <b>Jingjing Chen</b>, Zuxuan Wu, Yu-Gang Jiang<br> IEEE TMM, 2021.
				<br><br>
				</li>
				
				<li> Mixed Dish Recognition with Contextual Relation and Domain Alignment <a href="https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=7411&amp;context=sis_research">pdf</a><br> Lixi Deng, <b>Jingjing Chen*</b>, Chong-Wah Ngo, Qianru Sun, Sheng Tang, Yongdong Zhang, Tat-Seng Chua<br> IEEE TMM, 2021.
				<br><br>
				</li>
				
				<li> A Study of Multi-Task and Region-Wise Deep Learning for Food Ingredient Recognition <a href="https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=7304&amp;context=sis_research">pdf</a><br> <b>Jingjing Chen</b>, Bin Zhu, Chong-Wah Ngo, Tat-Seng Chua, Yu-Gang Jiang<br> IEEE TIP, 2020.
				<br><br>
				</li>
				
			    <li> A Hybrid Approach for Detecting Prerequisite Relations in Multi-modal Food Recipes <a href="https://ieeexplore.ieee.org/document/9288707">pdf</a><br> Liangming Pan, <b>Jingjing Chen*</b>, Shaoteng Liu, Chong-Wah Ngo, Min-Yen Kan, Tat-Seng Chua<br> IEEE TMM, 2020.
				<br><br>
				</li>
				
			    <li> Wilddeepfake: A challenging real-world dataset for deepfake detection <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413769">pdf</a><br> Bojia Zi, Minghao Chang, <b>Jingjing Chen</b>, Xingjun Ma, Yu-Gang Jiang<br> ACM Multimedia, 2020.
				<br><br>
				</li>
				
			    <li> Multi-modal cooking workflow construction for food recipes <a href="https://arxiv.org/abs/2008.09151">pdf</a><br> Liang-Ming Pan, <b>Jingjing Chen*</b>, Jianlong Wu, Shaoteng Liu, Chong-Wah Ngo, Min-Yen Kan, Yugang Jiang, Tat-Seng Chua<br> IEEE TMM, 2020.
				<br><br>
				</li>
				
			    <li> Person-level Action Recognition in Complex Events via TSD-TSM Networks <a href="https://dl.acm.org/doi/10.1145/3394171.3416276">pdf</a><br> Yanbin Hao, Zi-Niu Liu, Hao Zhang, Bin Zhu, <b>Jingjing Chen</b>, Yu-Gang Jiang, Chong-Wah Ngo<br> ACM Multimedia, 2020.
				<br><br>
				</li>
				
				 <li> Video Relation Detection via Multiple Hypothesis Association <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413764">pdf</a><br> Zixuan Su, Xindi Shang, <b>Jingjing Chen</b>, Yu-Gang Jiang, Zhiyong Qiu, Tat-Seng Chua<br> ACM Multimedia, 2020.
				<br><br>
				</li>
				
			    <li> Cross-domain Cross-modal Food Transfer <a href="https://dl.acm.org/doi/10.1145/3394171.3413809">pdf</a><br> Bin Zhu, Chong-Wah Ngo, <b>Jingjing Chen</b><br> ACM Multimedia, 2020.
				<br><br>
				</li>
				
			    <li>Visual Relations Augmented Cross-modal Retrieval <a href="https://dl.acm.org/doi/10.1145/3372278.3390709">pdf</a><br> Yutian Guo, <b>Jingjing Chen</b>, Hao Zhang, Yu-Gang Jiang<br> ACM ICMR, 2020.
				<br><br>
				</li>
				
			    <li> Hyperbolic Visual Embedding Learning for Zero-Shot Recognition <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Hyperbolic_Visual_Embedding_Learning_for_Zero-Shot_Recognition_CVPR_2020_paper.pdf">pdf</a><br> Shaoteng Liu, <b>Jingjing Chen*</b>, Liangming Pan, Chong-Wah Ngo, Tat-Seng Chua<br> CVPR, 2020.
				<br><br>
				</li>
			    
			    <li> Zero-shot Ingredient Recognition by Multi-Relational Graph Convolutional Network <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6626">pdf</a><br><b>Jingjing Chen</b>, Liangming Pan, Zhipeng Wei, Xiang Wang, Chong-Wah Ngo, Tat-Seng Chua<br> AAAI, New York, USA, February, 2020.
				<br><br>
				</li>
				
				<li> Heuristic Black-box Adversarial Attacks on Video Recognition Models <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6918/6772">pdf</a><br>Zhipeng Wei, <b>Jingjing Chen</b>, Xingxing Wei, Linxi Jiang, Tat-Seng Chua, FengFeng Zhou, Yu-Gang Jiang<br> AAAI, New York, USA, February, 2020.
				<br><br>
				</li>
				
				<li> Mixed-dish Recognition with Contextual Relation Networks <a href="https://dl.acm.org/doi/10.1145/3343031.3351147">pdf</a> <br>Lixi Deng, <b>Jingjing Chen*</b>, Xiangnan He, Qianru Sun, Sheng Tang, Zhaoyan Ming, Yongdong Zhang and Tat-Seng Chua<br> ACM Multimedia (ACM MM), Nice, France, October, 2019. 
				<br><br>
				</li>
				
				<li> DietLens-Eout: Large Scale Restaurant Food Photo Recognition <a href="https://dl.acm.org/doi/10.1145/3323873.3326923">pdf</a><br>Zhipeng Wei, <b>Jingjing Chen</b>, Zhaoyan Ming, Chong-Wah NGO, Tat-Seng Chua and Fengfeng Zhou<br> ICMR, Ottawa, Canada, June, 2019. 
				<br><br>
				</li>
				
				<li> Mix-dish Recognition Through Multi-label Learning <a href="https://dl.acm.org/doi/10.1145/3326458.3326929">pdf</a><br>Yunan Wang, <b>Jingjing Chen</b>, Chong-Wah NGO, Zhaoyan Ming, Tat-Seng Chua and Wanli Zuo<br> Workshop on Multimedia for Cooking and Eating Activities (CEA), Ottawa, Canada, June, 2019.
				<br><br>
				</li>
				
				<li> R2GAN: Cross-modal Recipe Retrieval with Generative Adversarial Networks <a href="http://vireo.cs.cityu.edu.hk/papers/R2GAN.pdf">pdf</a><br>Bin Zhu,Chong-Wah NGO, <b>Jingjing Chen</b> and Yanbin Hao<br> CVPR, Long Beach, CA, USA, June, 2019.
				<br><br>
				</li>
				
				<li> Deep Understanding of Cooking Procedures for Cross-modal Recipe Retrieval <a href="http://vireo.cs.cityu.edu.hk/papers/2018_p1020-chen.pdf">pdf</a><br><b>Jingjing Chen</b>, Chong-Wah NGO, Fuli Feng and Tat-Seng Chua<br> ACM Multimedia (ACM MM), Seoul, Korea, October, 2018.
				<br><br>
				</li>
				
				<li> Cross-modal Recipe Retrieval with Stacked Attention Model <a href="http://vireo.cs.cityu.edu.hk/papers/2018_CrossmodalRecipe.pdf">pdf</a><br><b>Jingjing Chen</b>, Lei Pang and Chong-Wah NGO<br> Multimedia Tools and Applications, Volume 77, 2018.
				<br><br>
				</li>
				
				<li> Food Photo Recognition for Dietary Tracking: System and Experiment <a href="http://vireo.cs.cityu.edu.hk/papers/Dietary.pdf">pdf</a><br>Zhaoyan Ming, <b>Jingjing Chen</b>, Yu Cao, Ciaran Forde, Chong-Wah NGO and Tat-Seng Chua<br> Multimedia Modeling (MMM), Bangkok, Thailand, January, 2018.
				<br><br>
				</li>
				
				<li> Cross-modal Recipe Retrieval with Rich Food Attributes <a href="http://vireo.cs.cityu.edu.hk/papers/jingjingmm2017.pdf">pdf</a><br><b>Jingjing Chen</b>, Chong-Wah NGO and Tat-Seng Chua<br> ACM Multimedia (ACM MM), Mountain View, CA, USA, October, 2017. <span style="color:red;">(oral)</span>
				<br><br>
				</li>
				
				<li> PIC2DISH: A Customized Cooking Assistant System <a href="https://dl.acm.org/citation.cfm?doid=3123266.3126490">pdf</a><br>Yongsheng An, Yu Cao, <b>Jingjing Chen</b>, Chong-Wah Ngo, Jia Jia, Huanbo Luan and Tat-Seng Chua<br> ACM Multimedia (ACM MM), Mountain View, CA, USA, October, 2017.
				<br><br>
				</li>
				
				<li> Cross-modal Recipe Retrieval: How to Cook This Dish? <a href="http://vireo.cs.cityu.edu.hk/jingjing/papers/chen2017cross.pdf">pdf</a><br><b>Jingjing Chen</b>, Lei Pang and Chong-Wah NGO<br> International Conference on Multimedia Modeling(MMM), Reykjavi, Iceland, January, 2017. <span style="color:red;">(Best Student Paper Award)</span>
				<br><br>
				</li>
				
				<li> Deep-based Ingredient Recognition for Cooking Recipe Retrieval <a href="http://vireo.cs.cityu.edu.hk/jingjing/papers/chen2016deep.pdf">pdf</a><br><b>Jingjing Chen</b> and Chong-Wah Ngo<br> ACM Multimedia (ACM MM), Amsterdam, Netherlands, October, 2016. <span style="color:red;">(Best Student Paper Award)</span><br><br>
				</li>

				<li> Image aesthetics enhancement using composition-based saliency detection <br>Handong Zhao, <b>Jingjing Chen</b>, Yahong Han and Xiaochun Cao<br> Multimedia System, vol. 21, no. 2, pp. 159-168, 2015
				<br><br>
				</li>

				<li>VIREO@TRECVID 2014: Instance Search and Semantic Indexing <a href="http://vireo.cs.cityu.edu.hk/papers/VIREO_TRECVID2014_ins_sin.pdf">pdf</a><br> Wei Zhang, Hao Zhang, Ting Yao, Yijie Lu, <b>Jingjing Chen</b> and Chong-Wah NGO<br>NIST TRECVID Workshop (TRECVID'14), Orlando, USA, November, 2014
				<br><br>
				</li>
				
				<li>Object Coding on the Semantic Graph for Scene Classification <br><b>Jingjing Chen</b>, Yahong Han, Xiaochun Cao and Qi Tian<br> ACM Multimedia (ACM MM), Barcelona, Spain, October 2013.
				<br><br>
				</li>

				<li>Visual Saliency Detection Based on Photographic Composition <br><b>Jingjing Chen</b>, Handong Zhao, Yahong Han and Xiaochun Cao<br>International Conference on Internet Multimedia Computing and Service (ICIMCS), huangshan, Anhui, China, August 2013. <span style="color:red;">(Best Paper Honorable Mention)</span>
				<br><br>
				</li>
				
				
				<li> Feature selection with Spatial Path Coding for Multimedia Analysis <br>Yahong Han, <b>Jingjing Chen</b> and Xiaochun Cao<br>Information Sciences, vol. 281, pp. 523-525, 2014
				<br><br>
				</li>
				
				<li>Object clique representation for scene classification <br><b>Jingjing Chen</b>, Xiaochun Cao and Bao Zhang<br>International Conference on Pattern Recognition (ICPR), Tsukuba, Japan, November, 2012.
				<br><br>
				</li>
				
				
			</ul>
		</div>

		<div class="content" id="teach">
			<h5 class="text-primary">Teaching</h5>
			<ul>
			  <li><strong>Instructor</strong>, COMP130124, Computer Vision, Fudan University, For Undergraduate Students, Fall 2021, 2022, 2023, 2024</li>
			  <li><strong>Instructor</strong>, COMP620028, Information Retrieval, Fudan University, For Graduate Students, Fall 2020, 2021, 2022, 2023, 2024</li>
			  <li><strong>Instructor</strong>, Advanced Media Computing, Fudan University, For Senior Undergraduate and Graduate Students, Summer Semester 2022</li>
			  <li><strong>Instructor</strong>, Frontiers of Computer Vision, For Senior Undergraduate and Graduate Students, Summer Semester 2022</li>
			</ul>
		</div>		

		<div class="content">
			<h5 class="text-primary">Academic Services</h5>
				<ul>
				  <li><strong>Associate Editor</strong>: IEEE TMM, ACM TOMM, Neurocomputing</li>
				  <li><strong>Editor</strong>: ACM SIGMM Records</li>
				  <li><strong>Technical Committee Member</strong>: IEEE-CAS Multimedia Systems & Applications (2021-2025)</li>
				  <li><strong>Conference Area Chair (AC)</strong>: 
				    <ul>
				      <li>ACM Multimedia (2020, 2024)</li>
				      <li>ACL (2023)</li>
				    </ul>
				  </li>
				  <li><strong>Senior Program Chair</strong>: AAAI (2023, 2024, 2025)</li>
				  <li><strong>Conference Organizer</strong>: 
				    <ul>
				      <li>ACM Multimedia 2021 (Registration Chair)</li>
				      <li>ACM ICMR 2022 (Publicity Chair)</li>
				      <li>ICME 2022 (Special Session Chair)</li>
				      <li>ACM Multimedia Asia 2023 (Publicity Chair)</li>
				    </ul>
				  </li>
				  <li><strong>Conference Reviewer</strong>: ACM MM, ICLR, CVPR, ICCV, ECCV, etc.</li>
				  <li><strong>Journal Reviewer</strong>: TPAMI, TIP, TKDE, TMM, TOMM, TCSVT, etc.</li>
				</ul>

		</div>

		<div class="content" id="team">
			
			<h5 class="text-primary">Group Members</h5>
		
			<strong>PhD Students:</strong>
			<ul>
			  <li>Jiarui Yang (September 2024 – present)</li>
			  <li>Yue Yu (September 2024 – present)</li>
			  <li>Guoshan Liu (September 2024 – present)</li>
			  <li>Xue Song (September 2022 – present)</li>
			  <li>Yang Jiao (September 2021 – present)</li>
			  <li>Pengkun Jiao (September 2021 – present)</li>
			  <li>Kai Chen (September 2021 – present)</li>
			</ul>
			
			<strong>Master Students:</strong>
				

		</div>

		<div class="content">
			<h5 class="text-primary">Former Members</h5>
				<ul>
				  <li>Shaoxiang Chen</li>
				</ul>
		</div>
			
		<div class="content">
			<h5 class="text-primary">Miscellaneous</h5>


			<p>When I have spare time, I enjoy cooking. </p>

			<p>
			</p>
		</div>
		Webpage template borrowed from Prof. <a href="https://cocoxu.github.io/#advise">Wei Xu</a>.

	
</body></html>
