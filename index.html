<!DOCTYPE html>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	
	<meta content="IE=edge" http-equiv="X-UA-Compatible">
	<meta content="width=840, initial-scale=1.0" name="viewport">
	<meta content="Jingjing Chen" name="author">
	<meta content="Homepage" name="description">

	<style>
	        strong {
	            color: rgb(235, 104, 100);
	        }
	    </style>

	<title>Jingjing Chen - Fudan - CS</title>
	<link rel="shortcut icon" href="https://www.ic.gatech.edu/sites/all/themes/coc_sub_theme/favicon.ico" type="image/vnd.microsoft.icon">
	<!-- Bootstrap core CSS -->
	<link href="./files/bootstrap.min.css" rel="stylesheet">
	<!-- Bootstrap theme -->
	<link href="./files/bootstrap-theme.min.css" rel="stylesheet">
	<!-- Bootstrap icon -->
	<link href="./files/bootstrap.icon-large.min.css" rel="stylesheet">
	<!-- Custom styles for this template -->
	<link href="./files/theme.css" rel="stylesheet">
	<!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
	<link rel="stylesheet" href="./files/font-awesome.min.css">

	<!--[if lt IE 9]>
	  <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
	  <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
	<![endif]-->
<script src="chrome-extension://njgehaondchbmjmajphnhlojfnbfokng/js/contentScripts/dom.js"></script></head>

<body>
   <script type="text/javascript" async="" src="./files/ga.js"></script>
	<div class="container-narrow">
		<div class="title">

			<h4><strong>Jingjing Chen</strong> &nbsp;&nbsp;&nbsp;&nbsp; 
			
			<img class="img-thumbnail-lizi" height="300" hspace="10" src="./files/jingjingchen_profile_image.jpg" style="float:right"> 
			
			</h4>
			Associate Professor<br>
			<a href="https://cs.fudan.edu.cn/">School of Computer Science</a> <br>
			<a href="https://www.fudan.edu.cn/">Fudan University</a><br>
			<span class="glyphicon glyphicon-envelope"></span>&nbsp; <tt>chenjingjing@fudan.edu.cn</tt><br>
			<b>Office: </b>No.2 Interdisciplinary Research Building D5021<br>
			<a href="https://scholar.google.com.hk/citations?user=DfWdqzQAAAAJ">[Google Scholar]</a>

		    <br><br>
		    	<p>I am currently an Associate Professor at the School of Computer Science, Fudan University. Prior to joining Fudan, I was a Research Fellow at the School of Computing, <a href="https://www.nus.edu.sg/">National University of Singapore (NUS)</a>, working with <a href="https://www.chuatatseng.com/">Prof. Tat-Seng Chua</a>. I received my Ph.D. degree in Computer Science from City University of Hong Kong in July 2018, supervised by <a href="https://computing.smu.edu.sg/faculty/profile/601/ngo-chong-wah">Prof. Chong-Wah Ngo</a>. I received several accolades, including “<strong>ACM SIGMM Rising Star Award</strong>” in 2024, “<strong>IEEE ICME Rising Star Runner Up</strong>” in 2023, “<strong>ACM Shanghai Rising Star Award</strong>” in 2020, the “<strong>AI 2000 Most Influential Scholar Nomination Award</strong>” in 2021, the “<strong>Best Paper Award</strong>” at ChinaMM 2021, the “<strong>Best Student Paper Award</strong>” at ACM Multimedia 2016, the “<strong>Best Student Paper Award</strong>” at Multimedia Modeling 2017. I’m also listed as “<strong>Stanford & Elsevier’s Top 2% Scientist</strong>” in 2024 etc. </p>

		        <p>My research primarily focuses on, but is not limited to:</p>
			<ol>
			  <li><strong>Multimedia Content Analysis</strong>: Cross-modal Retrieval, Image/Video Classification, Multi-modal Forgery Detection, Food Recognition, and Nutrition Analysis</li>
			  <li><strong>Multimedia Model Security</strong>: Adversarial Attack and Defense, Transferable Adversarial Attack, Backdoor Attack, Adversarial Training, Adversarial Robustness Evaluation for Large Multi-modal Models, Unsafe Concept Removal for Generative Models</li>
			</ol>


			<br>
						
			<img src="./files/recruitment.png" width="20" alt="" style="border-style: none" align="center"> &nbsp; 
			<strong>Open Positions</strong>
			<be>
				<ul>
				  <li>I am recruiting research fellow (holding a doctoral degree) with relevant research experience on Multimedia Contents Analysis and Generative AI. Candidates are expected to have publications in top conferences (NeurIPS, ICLR, ICML, CVPR, ECCV, ICCV, AAAI, ACM-MM, etc.) or high-impact journals (T-PAMI, TIP, TMM,  IJCV, etc.). Qualified candidates are welcome to apply.</li>
	
				  <li>I am looking for 2-3 interns working on Generative AI Security. Please feel free to email me with your CV if you are interested.</li>
				  <li>Currently, there are no openings for PhD students starting in Fall 2025.</li>
				</ul>
			<br> 
			

		</div>

<div class="navbar navbar-default" role="navigation">
		
   		<div class="container">
        
				<div class="navbar-collapse">
				<ul class="nav navbar-nav">
					<li>
						<a href="https://jingjing1.github.io/#research">Research</a>
					</li>

					<li>
						<a href="https://jingjing1.github.io/#team">Team</a>
					</li>


					<li>
						<a href="https://jingjing1.github.io/#publications">Publications</a>
					</li>


					<li>
						<a href="https://jingjing1.github.io/#teach">Teaching</a>
					</li>


					<li>
						<a href="https://jingjing1.github.io/#publications">Code &amp; Data</a>
					</li>
				</ul>

          
			</div>
		
		</div>
		</div>


		<div class="content">
			<h5 class="text-primary" id="news">What's New</h5>
		         
			<!-- <ul class="news"> -->
			<ul>
				<li style="list-style: none"><br>
				</li>

			<!-- <ul class="news"> -->
			   <li><strong>[Jul 2025]</strong> Our paper on Adversatial attack was accepted by IJCV!</li>
			  <li><strong>[Jul 2025]</strong> One paper about multi-modal large language models is accepted by ACM Multimedia 2025!</li>
			  <li><strong>[Jul 2025]</strong> One paper about multi-modal large language models is accepted by ICCV 2025!</li>
			  <li><strong>[May 2025]</strong> My PhD students Xue Song and Kai Chen have successfully defended their doctoral dissertations!</li>
			  <li><strong>[May 2025]</strong> My master students Wentao Tian, Yinxuan Gui, Jiacheng Zhang have successfully defended their master's dissertations!</li>
			  <li><strong>[Feb 2025]</strong> I will serve as the Social Media Co-Chair for ACM Multimedia 2026!</li>
			  <li><strong>[Feb 2025]</strong> Two papers are accepted by CVPR 2025! </li>
			  <li><strong>[Jan 2025]</strong> I will serve as the Program Chair for <a href="https://mmm2026.cz/organisation.html"> MMM 2026 </a>, which will be held in Prague!</li>
			  <li><strong>[Jan 2025]</strong> Our FoodLMM paper is accepted by TMM 2025!</li>
		          <li><strong>[Dec 2024]</strong> Two papers are accepted by AAAI 2025! </li>
			  <li><strong>[Nov 2024]</strong> My Ph.D. student, Zhipeng Wei (graduated in June 2024), has been awarded the CSIG Outstanding Doctoral Dissertation Award! <a href="https://mp.weixin.qq.com/s/jjrRQ2tlBL_SBXrBmhqj0g">[link]</a></li>
			  <li><strong>[Nov 2024]</strong> I am appointed as an Associate Editor for ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)!</li>
			  <li><strong>[Oct 2024]</strong> I am awarded ACM SIGMM Rising Star Award! <a href="http://www.sigmm.org/news/sigmm_rising_star_award_2024">[link]</a></li>
			  <li><strong>[Oct 2024]</strong> One paper about open-set single-source domain generalization is accepted by TMM 2024!</li>
			  <li><strong>[Oct 2024]</strong> I am appointed as an Associate Editor for IEEE Transactions on Multimedia (TMM)!</li>
			  <li><strong>[Oct 2024]</strong> My master’s students, Ziyi Gao and Junhao Xu, have been awarded the National Scholarship!</li>
			  <li><strong>[Sep 2024]</strong> One paper about multi-modal large language models is accepted by NeurIPS 2024!</li>
			  <li><strong>[Sep 2024]</strong> I am listed as “Stanford/Elsevier’s Top 2% Scientist in 2024!</li>
			  <li><strong>[July 2024]</strong> Five papers are accepted by ACM Multimedia 2024!</li>
			  <li><strong>[July 2024]</strong> Two papers are accepted by ECCV 2024!</li>
			  <li><strong>[May 2024]</strong> My PhD students Zhipeng Wei and Tianwen Qian have successfully defended their doctoral dissertations!</li>
			  <li><strong>[May 2024]</strong> My master students Yanqi Wu, Yue Yu, Wenzhuo Xu, Yiqiang Lv, Xueqing Zhou have successfully defended their master's dissertations!</li>
			</ul>		
		</div>




		<div class="content">
			<h5 class="text-primary" id="research">Research Highlights</h5> 
			
			<!-- Highlight 1 -->
			<div class="projects" style="margin-left: 1em; margin-right: 1em">
				<div class="boxed">
					<p><a href="./files/idforge_paper.png"><img border="1px" hspace="10" vspace="10" src="./files/idforge_paper.png" style="float: right;" width="350px"></a>
					</p>
		
			<h5><strong style="color: black;">Multimedia Forgery Detection</strong></h5>


			<p> We work on constructing high quality datasets for training and evaluating deepfake detection models. We published one of the earliest datasets <strong>WildDeepFake</strong> [ACM MM’20] for evaluating the performance of deepfake detection models against real-world deepfakes sourced from the internet. Recently, we introduced <strong>IDForge</strong> [ACM MM’24], the largest multimodal deepfake detection dataset to date, encompassing nine of the most popular manipulation types across visual, audio, and textual modalities. In addition to dataset constrcution, we also work on enhancing deepfake detection accuracy and generalization. This includes employing <strong>uncertainty-aware learning</strong> [ACM MM’23] and developing multimodal approaches like <strong>multimodal identity reference-assisted detection</strong> [ACM MM’24].</p>


				</div>
				<!-- end boxed-->

			</div>


			<!-- Highlight 2 -->
			<div class="projects" style="margin-left: 1em; margin-right: 1em">
				<div class="boxed">
					<p><a href="./files/paper_advre_mm.png"><img border="1px" hspace="10" vspace="10" src="./files/paper_advre_mm.png" style="float: left;" width="330px"></a>
					</p>
		
				<h5><strong style="color: black;">Adversarial Robustness Evaluation for Multimedia Models</strong></h5>


				<p>We work on efficient robustness adversarial evaluation approaches for complex multimedia models. We develop transfer-based adversarial attack methods which enhance the transferability through diverse input generation, such as temporal translation [AAAI 2022], random cropping [CVPR 2023], for black-box attack on video models. We also investigate the possibility of cross-modal attack [CVPR 2022] which crafts adversarial perturbations on images models for attacking video models, as well as task-agnostic attack which evaluate the robustness of pre-trained vision-language models (e.g., CLIP) [ICME 2023].  Beyond those restricted adversarial attack, we recently investigated un-restricted attack which directly generate adversarial examples using diffusion models to alter few pixels to perform attack on video models as well as VL models [ACM MM 2024 b, ACM MM 2024 c].  </p>


				</div>
				<!-- end boxed-->

			</div>
			

			<!-- Highlight 3 -->
			<div class="projects" style="margin-left: 1em; margin-right: 1em">
				<div class="boxed">
					<p><a href="./files/rgai_paper.png"><img border="1px" hspace="10" vspace="10" src="./files/rgai_paper.png" style="float: right;" width="330px"></a>
					</p>
		
				<h5><strong style="color: black;">Responsible Generative AI</strong></h5>


				<p>We recently published two papers on removing unsafe concepts in diffusion models. In ECCV 2024, we introduced <strong>Reliable and Efficient Concept Erasure (RECE)</strong>, a novel approach that enables modification of the model within 3 seconds, without requiring additional fine-tuning. Additionally, we have another work - <strong>Dual encoder Modulation network (DuMo)</strong>, which achieves precise erasure of inappropriate target concepts with minimum impairment to non-target concept. This work has been accepted for AAAI 2025. In addition to unsafe concept removal, we are also focusing on AI-generated content identification and <strong>Offensive and Non-compliant Content Attribution</strong>, with the goal of fostering responsible generative AI. These works will be available on arXiv soon.</p>


				</div>
				<!-- end boxed-->

			</div>

			<!-- Highlight 4 -->
			<div class="projects" style="margin-left: 1em; margin-right: 1em">
				<div class="boxed">
					<p><a href="./files/aihealth_paper.png"><img  border="1px" hspace="10" vspace="10" src="./files/aihealth_paper.png" style="float: left;" width="420px"></a>
					</p>
		
				<h5><strong style="color: black;">AI + Health (Interdisciplinary Research)</strong></h5>


				<p> We work on an interesting and useful application that aim to improve human health using AI technologies. We published one of the earliest works on Large Multi-modal Model in food domain, named as FoodLMM [accepted by TMM 2025]. Moreover, we also work on Weight and Health Prediction Based on Food Logs [ACM MM 2024], Food Image and Nutrition Analysis, Diet Suggestion Generation Using Large Multimodal Models (LMMs)) etc. </p>


				</div>
				<!-- end boxed-->

			</div>
		</div>

		 
				    
				    
		<div class="content">
			<h5 class="text-primary" id="publications">Publications</h5>

			<ul>
				<li> <b> DuMo: Dual Encoder Modulation Network for Precise Concept Erasure </b> &nbsp  <br>Feng Han, Kai Chen, Chao Gong, Zhipeng Wei, Jingjing Chen, Yu-Gang Jiang <br>The Association for the Advancement of Artificial Intelligence (<b>AAAI</b>), 2025 <br><br></li>
				<li> <b> FaceA-Net: Facial Attribute-driven ID Preserving Image Generation Network </b> &nbsp  <br>Jiayu Wang, Yue Yu, <b>Jingjing Chen</b>, Qi Dai, Yu-Gang Jiang <br>The Association for the Advancement of Artificial Intelligence (<b>AAAI</b>), 2025 <br><br></li>
				<li> <b> Retrieval Augmented Recipe Generation </b> &nbsp <a href="https://arxiv.org/pdf/2411.08715?">[pdf]</a> &nbsp <br>Guoshan Liu,Hailong Yin,Bin Zhu,<b>Jingjing Chen</b>,Chong-Wah Ngo,Yu-Gang Jiang <br>IEEE/CVF Winter Conference on Applications of Computer Vision (<b>WACV</b>) 2025 <br><br></li>
				<li> <b> Lumen: Unleashing Versatile Vision-Centric Capabilities of Large Multimodal Models </b> &nbsp <a href="https://openreview.net/pdf?id=v5Un2QqnRf">[pdf]</a> &nbsp <br>Yang Jiao, Shaoxiang Chen, Zequn Jie, <b>Jingjing Chen</b>, Lin Ma, Yu-Gang Jiang <br>Conference on Neural Information Processing Systems (<b> NeurIPS </b>), 2024 <br><br></li>
				<li> <b> Domain Expansion and Boundary Growth for Open-Set Single-Source Domain Generalization </b> &nbsp <a href="https://arxiv.org/pdf/2411.02920">[pdf]</a> &nbsp <br>Pengkun Jiao, Na Zhao, <b>Jingjing Chen</b>, Yu-Gang Jiang <br>IEEE Transactions on Multimedia (<b>TMM</b>), 2024. <br><br></li>
				<li> <b> Identity-Driven Multimedia Forgery Detection via Reference Assistance </b> &nbsp  <br>Junhao Xu, <b>Jingjing Chen</b>, Xue Song, Hanjun Shan, Yu-Gang Jiang <br>ACM International Conference on Multimedia (<b>ACM MM</b>), 2024. <br><br></li>
				<li> <b> ReToMe-VA: Recursive Token Merging for Video Diffusion-based Unrestricted Adversarial Attack </b> &nbsp <a href="https://dl.acm.org/doi/10.1145/3664647.3680959">[pdf]</a> &nbsp <br>Ziyi Gao, Kai Chen, Zhipeng Wei, Tingshu Mou, <b>Jingjing Chen</b>, Zhiyu Tan, Hao Chen, Yu-Gang Jiang <br>ACM International Conference on Multimedia (<b>ACM MM</b>), 2024. <br><br></li>
				<li> <b> Highly Transferable Diffusion-based Unrestricted Adversarial Attack on Pre-trained Vision-Language Models </b> &nbsp <a href="https://openreview.net/pdf?id=yAygQe3Uxd">[pdf]</a> &nbsp <br>Wenzhuo Xu, Kai Chen, Ziyi Gao, Zhipeng Wei, <b>Jingjing Chen</b>, Yu-Gang Jiang <br>ACM International Conference on Multimedia (<b>ACM MM</b>), 2024. <br><br></li>
				<li> <b> Navigating Weight Prediction with Diet Diary </b> &nbsp  <br>Yinxuan Gui, Bin Zhu, <b>Jingjing Chen</b>, Chong-Wah Ngo, Yu-Gang Jiang <br>ACM International Conference on Multimedia (<b>ACM MM</b>), 2024. <br><br></li>
				<li> <b> Reliable and Efficient Concept Erasure of Text-to-Image Diffusion Models </b> &nbsp  <br>Chao Gong, Kai Chen, Zhipeng Wei, <b>Jingjing Chen</b>, Yu-Gang Jiang <br>European Conference on Computer Vision (<b>ECCV</b>), 2024. <br><br></li>
				<li> <b> Unlocking Textual and Visual Wisdom: Open-Vocabulary 3D Object Detection Enhanced by Comprehensive Guidance from Text and Image </b> &nbsp <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06480.pdf">[pdf]</a> &nbsp <br>Pengkun Jiao, Na Zhao, <b>Jingjing Chen</b>, Yu-Gang Jiang <br>European Conference on Computer Vision (<b>ECCV</b>), 2024. <br><br></li>
				<li> <b> Doubly Abductive Counterfactual Inference for Text-based Image Editing </b> &nbsp <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Song_Doubly_Abductive_Counterfactual_Inference_for_Text-based_Image_Editing_CVPR_2024_paper.pdf">[pdf]</a> &nbsp<a href="https://github.com/xuesong39/DAC">[code]</a> <br>Xue Song, Jiequan Cui, Hanwang Zhang, <b>Jingjing Chen</b>, Richang Hong, Yu-Gang Jiang <br>In IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024. <br><br></li>
				<li> <b> NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario </b> &nbsp <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28253/28499">[pdf]</a> &nbsp <br>Tainwen Qian, <b>Jingjing Chen</b>, Linhai Zhuo, Yang Jiao, Yu-Gang Jiang <br>In The 38th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2024. <br><br></li>
				<li> <b> Open-Vocabulary Video Relation Extraction </b> &nbsp <a href="https://arxiv.org/pdf/2312.15670">[pdf]</a> &nbsp <br>Wentao Tian, Zheng Wang, Yuqian Fu, <b>Jingjing Chen</b>, Lechao Cheng <br>In The 38th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2024. <br><br></li>
				<li> <b> Instance-aware Multi-Camera 3D Object Detection with Structural Priors Mining and Self-Boosting Learning </b> &nbsp <a href="https://arxiv.org/abs/2312.08004">[pdf]</a> &nbsp <br>Yang Jiao, Zequn Jie, Shaoxiang Chen, Lechao Cheng, <b>Jingjing Chen</b>, Lin Ma, Yu-Gang Jiang <br>In The 38th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2024. <br><br></li>
				<li> <b> From Canteen Food to Daily Meals: Generalizing Food Recognition to More Practical Scenarios </b> &nbsp <a href="https://arxiv.org/pdf/2403.07403">[pdf]</a> &nbsp <br>Guoshan Liu, Yang Jiao, <b>Jingjing Chen</b>, Bin Zhu, Yu-Gang Jiang <br>IEEE Transactions on Multimedia (<b>TMM</b>, 2024. <br><br></li>
				<li> <b> Enhancing the Self-Universality for Transferable Targeted Attacks </b> &nbsp <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Enhancing_the_Self-Universality_for_Transferable_Targeted_Attacks_CVPR_2023_paper.pdf">[pdf]</a> &nbsp <br>Zhipeng Wei, <b>Jingjing Chen</b>, Zuxuan Wu, Yu-Gang Jiang <br>In IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2023. <br><br></li>
				<li> <b> MSMDfusion: Fusing lidar and camera at multiple scales with multi-depth seeds for 3D object detection </b> &nbsp <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiao_MSMDFusion_Fusing_LiDAR_and_Camera_at_Multiple_Scales_With_Multi-Depth_CVPR_2023_paper.pdf">[pdf]</a> &nbsp <br>Yang Jiao, Zequn Jie, Shaoxiang Chen, <b>Jingjing Chen</b>, Lin Ma, Yu-Gang Jiang <br>In IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2023. <br><br></li>
				<li> <b> SVFormer: Semi-Supervised Video Transformer for Action Recognition </b> &nbsp <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xing_SVFormer_Semi-Supervised_Video_Transformer_for_Action_Recognition_CVPR_2023_paper.pdf">[pdf]</a> &nbsp <br>Zhen Xing, Qi Dai, Han Hu, <b>Jingjing Chen</b>, Zuxuan Wu, Yu-Gang Jiang <br>In IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2023. <br><br></li>
				<li> <b> Relation Triplet Construction for Cross-modal Text-to-Video Retrieval </b> &nbsp <a href="https://dl.acm.org/doi/pdf/10.1145/3581783.3611940">[pdf]</a> &nbsp <br>Xue Song, <b>Jingjing Chen</b>, Yu-Gang Jiang <br>In ACM International Conference on Multimedia (<b>ACM MM</b>), 2023. <br><br></li>
				<li> <b> Generalizing Face Forgery Detection via Uncertainty Learning </b> &nbsp <a href="https://web.archive.org/web/20231028155715id_/https://dl.acm.org/doi/pdf/10.1145/3581783.3612102">[pdf]</a> &nbsp <br>Yanqi Wu, Xue Song, <b>Jingjing Chen</b>, Yu-Gang Jiang <br>In ACM International Conference on Multimedia (<b>ACM MM</b>), 2023. <br><br></li>
				<li> <b> Suspected Objects Matter: Rethinking Model's Prediction for One-stage Visual Grounding </b> &nbsp <a href="https://arxiv.org/pdf/2203.05186">[pdf]</a> &nbsp <br>Yang Jiao, Zequn Jie, <b>Jingjing Chen</b>, Lin Ma, Yu-Gang Jiang <br>In ACM International Conference on Multimedia (<b>ACM MM</b>), 2023. <br><br></li>
				<li> <b> GCMA: Generative Cross-Modal Transferable Adversarial Attacks from Images to Videos </b> &nbsp <a href="https://web.archive.org/web/20231027220136id_/https://dl.acm.org/doi/pdf/10.1145/3581783.3612110">[pdf]</a> &nbsp <br>Kai Chen, Zhipeng Wei, <b>Jingjing Chen</b>, Zuxuan Wu, Yu-Gang Jiang <br>In ACM International Conference on Multimedia (<b>ACM MM</b>), 2023. <br><br></li>
				<li> <b> On the Importance of Spatial Relations for Few-shot Action Recognition </b> &nbsp <a href="https://arxiv.org/pdf/2308.07119">[pdf]</a> &nbsp <br>Yilun Zhang, Yuqian Fu, Xingjun Ma, Lizhe Qi, <b>Jingjing Chen</b>, Zuxuan Wu, Yu-Gang Jiang <br>In ACM International Conference on Multimedia (<b>ACM MM</b>), 2023. <br><br></li>
				<li> <b> Towards transferable adversarial attacks on image and video transformers </b> &nbsp <a href="https://ieeexplore.ieee.org/abstract/document/10319323">[pdf]</a> &nbsp <br>Zhipeng Wei, <b>Jingjing Chen</b>, Micah Goldblum, Zuxuan Wu, Tom Goldstein, Yu-Gang Jiang, Larry S Davis <br>IEEE Transactions on Image Processing (<b>TIP</b>), 2023. <br><br></li>
				<li> <b> Downstream Task-agnostic Transferable Attacks on Language-Image Pre-training Models </b> &nbsp <a href="https://ieeexplore.ieee.org/abstract/document/10219910">[pdf]</a> &nbsp <br>Yiqiang Lv, <b>Jingjing Chen</b>, Zhipeng Wei, Kai Chen, Zuxuan Wu, Yu-Gang Jiang <br>In IEEE International Conference on Multimedia and Expo (<b>ICME</b>), 2023. <br><br></li>
				<li> <b> Unifying cross-lingual and cross-modal modeling towards weakly supervised multilingual vision-language pre-training </b> &nbsp <a href="https://aclanthology.org/2023.acl-long.327.pdf">[pdf]</a> &nbsp <br>Zejun Li, Zhihao Fan, <b>Jingjing Chen</b>, Qi Zhang, Xuan-Jing Huang, Zhongyu Wei <br>In Annual Meeting of the Association for Computational Linguistics (<b>ACL</b>), 2023. <br><br></li>
				<li> <b> Adaptive Split-Fusion Transformer </b> &nbsp <a href="https://arxiv.org/pdf/2204.12196">[pdf]</a> &nbsp <br>Zixuan Su, <b>Jingjing Chen</b>, Lei Pang, Chong-Wah Ngo, Yu-Gang Jiang <br>In IEEE International Conference on Multimedia and Expo (<b>ICME</b>), 2023. <br><br></li>
				<li> <b> Adaptive Cross-Modal Transferable Adversarial Attacks From Images to Videos </b> &nbsp <a href="https://ieeexplore.ieee.org/abstract/document/10375740">[pdf]</a> &nbsp <br>Zhipeng Wei, <b>Jingjing Chen</b>, Zuxuan Wu, Yu-Gang Jiang <br>IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2023. <br><br></li>
				<li> <b> Knowledge driven weights estimation for large-scale few-shot image recognition </b> &nbsp <a href="https://drive.google.com/file/d/10K6n2-65BHQsmXJATjnOgrviWZnvXdX2/view">[pdf]</a> &nbsp <br><b>Jingjing Chen</b>*, Linhai Zhuo*, Zhipeng Wei, Hao Zhang, Huazhu Fu, Yu-Gang Jiang <br>Pattern Recognition (<b>PR</b>), 2023. <br><br></li>
				<li> <b> Bic-net: Learning efficient spatio-temporal relation for text-video retrieval </b> &nbsp <a href="https://arxiv.org/pdf/2110.15609">[pdf]</a> &nbsp <br>Ning Han, Yawen Zeng, Chuhao Shi, Guangyi Xiao, Hao Chen, <b>Jingjing Chen</b> <br>ACM Transactions on Multimedia Computing, Communications and Applications (<b>TOMM</b>), 2023. <br><br></li>
				<li> <b> Hcms: Hierarchical and conditional modality selection for efficient video recognition </b> &nbsp <a href="https://arxiv.org/pdf/2104.09760">[pdf]</a> &nbsp <br>Zejia Weng, Zuxuan Wu, Hengduo Li, <b>Jingjing Chen</b>, Yu-Gang Jiang <br>ACM Transactions on Multimedia Computing, Communications and Applications (<b>TOMM</b>), 2023. <br><br></li>
				<li> <b> Enhancing the Self-Universality for Transferable Targeted Attacks  </b> &nbsp <a href="https://arxiv.org/abs/2209.03716">[pdf]</a> &nbsp <br>Zhipeng Wei, <b>Jingjing Chen</b>, Zuxuan Wu, Yu-Gang Jiang <br>CVPR, 2023. <br><br></li>
				<li> <b> SVFormer: Semi-Supervised Video Transformer for Action Recognition  </b> &nbsp <a href="https://arxiv.org/abs/2211.13222">[pdf]</a> &nbsp <br>Zhen Xing, Qi Dai, Han Hu, <b>Jingjing Chen</b>, Zuxuan Wu, Yu-Gang Jiang <br>CVPR, 2023. <br><br></li>
				<li> <b> Cross-Modal Transferable Adversarial Attacks from Images to Videos  </b> &nbsp <a href="https://arxiv.org/abs/2112.05379v1">[pdf]</a> &nbsp <br>Zhipeng Wei, <b>Jingjing Chen</b>, Zuxuan Wu, Yu-Gang Jiang <br>CVPR, 2022. <br><br></li>
				<li> <b> Balanced Contrastive Learning for Long-Tailed Visual Recognition  </b> &nbsp <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhu_Balanced_Contrastive_Learning_for_Long-Tailed_Visual_Recognition_CVPR_2022_paper.html">[pdf]</a> &nbsp <br>Jianggang Zhu, Zheng Wang, <b>Jingjing Chen</b>, Yi-Ping Phoebe Chen, Yu-Gang Jiang <br>CVPR, 2022. <br><br></li>
				<li> <b> ObjectFormer for Image Manipulation Detection and Localization  </b> &nbsp <a href="https://arxiv.org/abs/2203.14681">[pdf]</a> &nbsp <br>Junke Wang, Zuxuan Wu, <b>Jingjing Chen</b>, Xintong Han, Abhinav Shrivastava, Ser-Nam Lim, Yu-Gang Jiang <br>CVPR 2022. <br><br></li>
				<li> <b> MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes  </b> &nbsp <a href="https://arxiv.org/abs/2203.05203">[pdf]</a> &nbsp <br>Yang Jiao, Shaoxiang Chen, Zequn Jie, <b>Jingjing Chen</b>, Lin Ma, Yu-Gang Jiang <br>ECCV 2022. <br><br></li>
				<li> <b> Generalized Meta-FDMixup: Cross-Domain Few-Shot Learning Guided by Labeled Target Data  </b> &nbsp <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9942934">[pdf]</a> &nbsp <br>Yuqian Fu, Yanwei Fu, <b>Jingjing Chen</b>, Yu-Gang Jiang <br>IEEE TIP, 2022. <br><br></li>
				<li> <b> TGDM: Target Guided Dynamic Mixup for Cross-Domain Few-Shot Learning  </b> &nbsp <a href="http://arxiv.org/abs/2210.05392">[pdf]</a> &nbsp <br>Linhai Zhuo, Yuqian Fu, <b>Jingjing Chen</b>, Yixin Cao, Yu-Gang Jiang <br>ACM Multimedia, 2022. <br><br></li>
				<li> <b> ME-D2N: Multi-Expert Domain Decompositional Network for Cross-Domain Few-Shot Learning  </b> &nbsp <a href="https://arxiv.org/abs/2210.05280">[pdf]</a> &nbsp <br>Yuqian Fu, Yu Xie, Yanwei Fu, <b>Jingjing Chen</b>, Yu-Gang Jiang <br>ACM Multimedia, 2022. <br><br></li>
				<li> <b> Mix-DANN and Dynamic-Modal-Distillation for Video Domain Adaptation  </b> &nbsp <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548313">[pdf]</a> &nbsp <br>Yuehao Yin, Bin Zhu, <b>Jingjing Chen</b>, Lechao Cheng, Yu-Gang Jiang <br>ACM Multimedia, 2022. <br><br></li>
				<li> <b> Boosting the Transferability of Video Adversarial Examples via Temporal Translation  </b> &nbsp <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20168/19927">[pdf]</a> &nbsp <br>Zhipeng Wei, <b>Jingjing Chen</b>*, Zuxuan Wu, Yu-Gang Jiang* <br>AAAI, 2022. <br><br></li>
				<li> <b> Attacking Video Recognition Models with Bullet-Screen Comments  </b> &nbsp <a href="https://ojs.aaai.org/index.php/AAAI/article/view/19907/19666">[pdf]</a> &nbsp <br>Kai Chen, Zhipeng Wei, <b>Jingjing Chen</b>*, Zuxuan Wu, Yu-Gang Jiang* <br>AAAI, 2022. <br><br></li>
				<li> <b> Towards transferable adversarial attacks on vision transformers  </b> &nbsp <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20169/19928">[pdf]</a> &nbsp <br>Zhipeng Wei, <b>Jingjing Chen</b>, Micah Goldblum, Zuxuan Wu, Tom Goldstein, Yu-Gang Jiang <br>AAAI, 2022. <br><br></li>
				<li> <b> Two-stage Visual Cues Enhancement Network for Referring Image Segmentation  </b> &nbsp <a href="https://arxiv.org/pdf/2110.04435.pdf">[pdf]</a> &nbsp <br>Yang Jiao, Zequn Jie, Weixin Luo, <b>Jingjing Chen</b>*, Yu-Gang Jiang, Xiaolin Wei, Lin Ma* <br>ACM Multimedia, 2021. <br><br></li>
				<li> <b> Visual Co-Occurrence Alignment Learning for Weakly-Supervised Video Moment Retrieval  </b> &nbsp <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475278">[pdf]</a> &nbsp <br>Zheng Wang, <b>Jingjing Chen</b>, Yu-Gang Jiang <br>ACM Multimedia, 2021. <br><br></li>
				<li> <b> Fine-grained Cross-modal Alignment Network for Text-Video Retrieval  </b> &nbsp <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475241">[pdf]</a> &nbsp <br>Ning Han, <b>Jingjing Chen</b>, Guangyi Xiao, Hao Zhang, Yawen Zeng, Hao Chen <br>ACM Multimedia, 2021. <br><br></li>
				<li> <b> VideoLT: Large-scale Long-tailed Video Recognition  </b> &nbsp <a href="https://arxiv.org/abs/2105.02668">[pdf]</a> &nbsp <br>Xing Zhang, Zuxuan Wu, Zejia Weng, Huazhu Fu, <b>Jingjing Chen</b>, Yu-Gang Jiang, Larry Davis <br>ICCV, 2021. <br><br></li>
				<li> <b> Spatial-temporal Graphs for Cross-modal Text2Video Retrieval  </b> &nbsp <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413769">[pdf]</a> &nbsp <br>Xue Song, <b>Jingjing Chen</b>, Zuxuan Wu, Yu-Gang Jiang <br>IEEE TMM, 2021. <br><br></li>
				<li> <b> Mixed Dish Recognition with Contextual Relation and Domain Alignment  </b> &nbsp <a href="https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=7411&amp;context=sis_research">[pdf]</a> &nbsp <br>Lixi Deng, <b>Jingjing Chen</b>*, Chong-Wah Ngo, Qianru Sun, Sheng Tang, Yongdong Zhang, Tat-Seng Chua <br>IEEE TMM, 2021. <br><br></li>
				<li> <b> A Study of Multi-Task and Region-Wise Deep Learning for Food Ingredient Recognition  </b> &nbsp <a href="https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=7304&amp;context=sis_research">[pdf]</a> &nbsp <br><b>Jingjing Chen</b>, Bin Zhu, Chong-Wah Ngo, Tat-Seng Chua, Yu-Gang Jiang <br>IEEE TIP, 2020. <br><br></li>
				<li> <b> A Hybrid Approach for Detecting Prerequisite Relations in Multi-modal Food Recipes  </b> &nbsp <a href="https://ieeexplore.ieee.org/document/9288707">[pdf]</a> &nbsp <br>Liangming Pan, <b>Jingjing Chen</b>*, Shaoteng Liu, Chong-Wah Ngo, Min-Yen Kan, Tat-Seng Chua <br>IEEE TMM, 2020. <br><br></li>
				<li> <b> Wilddeepfake: A challenging real-world dataset for deepfake detection  </b> &nbsp <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413769">[pdf]</a> &nbsp <br>Bojia Zi, Minghao Chang, <b>Jingjing Chen</b>, Xingjun Ma, Yu-Gang Jiang <br>ACM Multimedia, 2020. <br><br></li>
				<li> <b> Multi-modal cooking workflow construction for food recipes  </b> &nbsp <a href="https://arxiv.org/abs/2008.09151">[pdf]</a> &nbsp <br>Liang-Ming Pan, <b>Jingjing Chen</b>*, Jianlong Wu, Shaoteng Liu, Chong-Wah Ngo, Min-Yen Kan, Yugang Jiang, Tat-Seng Chua <br>IEEE TMM, 2020. <br><br></li>
				<li> <b> Person-level Action Recognition in Complex Events via TSD-TSM Networks  </b> &nbsp <a href="https://dl.acm.org/doi/10.1145/3394171.3416276">[pdf]</a> &nbsp <br>Yanbin Hao, Zi-Niu Liu, Hao Zhang, Bin Zhu, <b>Jingjing Chen</b>, Yu-Gang Jiang, Chong-Wah Ngo <br>ACM Multimedia, 2020. <br><br></li>
				<li> <b> Video Relation Detection via Multiple Hypothesis Association  </b> &nbsp <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413764">[pdf]</a> &nbsp <br>Zixuan Su, Xindi Shang, <b>Jingjing Chen</b>, Yu-Gang Jiang, Zhiyong Qiu, Tat-Seng Chua <br>ACM Multimedia, 2020. <br><br></li>
				<li> <b> Cross-domain Cross-modal Food Transfer  </b> &nbsp <a href="https://dl.acm.org/doi/10.1145/3394171.3413809">[pdf]</a> &nbsp <br>Bin Zhu, Chong-Wah Ngo, <b>Jingjing Chen</b> <br>ACM Multimedia, 2020. <br><br></li>
				<li> <b> Visual Relations Augmented Cross-modal Retrieval  </b> &nbsp <a href="https://dl.acm.org/doi/10.1145/3372278.3390709">[pdf]</a> &nbsp <br>Yutian Guo, <b>Jingjing Chen</b>, Hao Zhang, Yu-Gang Jiang <br>ACM ICMR, 2020. <br><br></li>
				<li> <b> Hyperbolic Visual Embedding Learning for Zero-Shot Recognition  </b> &nbsp <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Hyperbolic_Visual_Embedding_Learning_for_Zero-Shot_Recognition_CVPR_2020_paper.pdf">[pdf]</a> &nbsp <br>Shaoteng Liu, <b>Jingjing Chen</b>*, Liangming Pan, Chong-Wah Ngo, Tat-Seng Chua <br>CVPR, 2020. <br><br></li>
				<li> <b> Zero-shot Ingredient Recognition by Multi-Relational Graph Convolutional Network  </b> &nbsp <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6626">[pdf]</a> &nbsp <br><b>Jingjing Chen</b>, Liangming Pan, Zhipeng Wei, Xiang Wang, Chong-Wah Ngo, Tat-Seng Chua <br>AAAI, New York, USA, February, 2020. <br><br></li>
				<li> <b> Heuristic Black-box Adversarial Attacks on Video Recognition Models  </b> &nbsp <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6918/6772">[pdf]</a> &nbsp <br>Zhipeng Wei, <b>Jingjing Chen</b>, Xingxing Wei, Linxi Jiang, Tat-Seng Chua, FengFeng Zhou, Yu-Gang Jiang <br>AAAI, New York, USA, February, 2020. <br><br></li>
				<li> <b> Mixed-dish Recognition with Contextual Relation Networks  </b> &nbsp <a href="https://dl.acm.org/doi/10.1145/3343031.3351147">[pdf]</a> &nbsp <br>Lixi Deng, <b>Jingjing Chen</b>*, Xiangnan He, Qianru Sun, Sheng Tang, Zhaoyan Ming, Yongdong Zhang and Tat-Seng Chua <br>ACM Multimedia (ACM MM), Nice, France, October, 2019. <br><br></li>
				<li> <b> DietLens-Eout: Large Scale Restaurant Food Photo Recognition  </b> &nbsp <a href="https://dl.acm.org/doi/10.1145/3323873.3326923">[pdf]</a> &nbsp <br>Zhipeng Wei, <b>Jingjing Chen</b>, Zhaoyan Ming, Chong-Wah NGO, Tat-Seng Chua and Fengfeng Zhou <br>ICMR, Ottawa, Canada, June, 2019. <br><br></li>
				<li> <b> Mix-dish Recognition Through Multi-label Learning  </b> &nbsp <a href="https://dl.acm.org/doi/10.1145/3326458.3326929">[pdf]</a> &nbsp <br>Yunan Wang, <b>Jingjing Chen</b>, Chong-Wah NGO, Zhaoyan Ming, Tat-Seng Chua and Wanli Zuo <br>Workshop on Multimedia for Cooking and Eating Activities (CEA), Ottawa, Canada, June, 2019. <br><br></li>
				<li> <b> R2GAN: Cross-modal Recipe Retrieval with Generative Adversarial Networks  </b> &nbsp <a href="http://vireo.cs.cityu.edu.hk/papers/R2GAN.pdf">[pdf]</a> &nbsp <br>Bin Zhu,Chong-Wah NGO, <b>Jingjing Chen</b> and Yanbin Hao <br>CVPR, Long Beach, CA, USA, June, 2019. <br><br></li>
				<li> <b> Deep Understanding of Cooking Procedures for Cross-modal Recipe Retrieval  </b> &nbsp <a href="http://vireo.cs.cityu.edu.hk/papers/2018_p1020-chen.pdf">[pdf]</a> &nbsp <br><b>Jingjing Chen</b>, Chong-Wah NGO, Fuli Feng and Tat-Seng Chua <br>ACM Multimedia (ACM MM), Seoul, Korea, October, 2018. <br><br></li>
				<li> <b> Cross-modal Recipe Retrieval with Stacked Attention Model  </b> &nbsp <a href="http://vireo.cs.cityu.edu.hk/papers/2018_CrossmodalRecipe.pdf">[pdf]</a> &nbsp <br><b>Jingjing Chen</b>, Lei Pang and Chong-Wah NGO <br>Multimedia Tools and Applications, Volume 77, 2018. <br><br></li>
				<li> <b> Food Photo Recognition for Dietary Tracking: System and Experiment  </b> &nbsp <a href="http://vireo.cs.cityu.edu.hk/papers/Dietary.pdf">[pdf]</a> &nbsp <br>Zhaoyan Ming, <b>Jingjing Chen</b>, Yu Cao, Ciaran Forde, Chong-Wah NGO and Tat-Seng Chua <br>Multimedia Modeling (MMM), Bangkok, Thailand, January, 2018. <br><br></li>
				<li> <b> Cross-modal Recipe Retrieval with Rich Food Attributes  </b> &nbsp <a href="http://vireo.cs.cityu.edu.hk/papers/jingjingmm2017.pdf">[pdf]</a> &nbsp <br><b>Jingjing Chen</b>, Chong-Wah NGO and Tat-Seng Chua <br>ACM Multimedia (ACM MM), Mountain View, CA, USA, October, 2017. <span style="color:red;">(oral)</span> <br><br></li>
				<li> <b> PIC2DISH: A Customized Cooking Assistant System  </b> &nbsp <a href="https://dl.acm.org/citation.cfm?doid=3123266.3126490">[pdf]</a> &nbsp <br>Yongsheng An, Yu Cao, <b>Jingjing Chen</b>, Chong-Wah Ngo, Jia Jia, Huanbo Luan and Tat-Seng Chua <br>ACM Multimedia (ACM MM), Mountain View, CA, USA, October, 2017. <br><br></li>
				<li> <b> Cross-modal Recipe Retrieval: How to Cook This Dish?  </b> &nbsp <a href="http://vireo.cs.cityu.edu.hk/jingjing/papers/chen2017cross.pdf">[pdf]</a> &nbsp <br><b>Jingjing Chen</b>, Lei Pang and Chong-Wah NGO <br>International Conference on Multimedia Modeling(MMM), Reykjavi, Iceland, January, 2017. <span style="color:red;">(Best Student Paper Award)</span> <br><br></li>
				<li> <b> Deep-based Ingredient Recognition for Cooking Recipe Retrieval  </b> &nbsp <a href="http://vireo.cs.cityu.edu.hk/jingjing/papers/chen2016deep.pdf">[pdf]</a> &nbsp <br><b>Jingjing Chen</b> and Chong-Wah Ngo <br>ACM Multimedia (ACM MM), Amsterdam, Netherlands, October, 2016. <span style="color:red;">(Best Student Paper Award)</span> <br><br></li>
				<li> <b> Image aesthetics enhancement using composition-based saliency detection </b> &nbsp  <br>Handong Zhao, <b>Jingjing Chen</b>, Yahong Han and Xiaochun Cao <br>Multimedia System, vol. 21, no. 2, pp. 159-168, 2015 <br><br></li>
				<li> <b> VIREO@TRECVID 2014: Instance Search and Semantic Indexing  </b> &nbsp <a href="http://vireo.cs.cityu.edu.hk/papers/VIREO_TRECVID2014_ins_sin.pdf">[pdf]</a> &nbsp <br>Wei Zhang, Hao Zhang, Ting Yao, Yijie Lu, <b>Jingjing Chen</b> and Chong-Wah NGO <br>NIST TRECVID Workshop (TRECVID'14), Orlando, USA, November, 2014 <br><br></li>
				<li> <b> Object Coding on the Semantic Graph for Scene Classification </b> &nbsp <a href="https://www.researchgate.net/profile/Yahong-Han-2/publication/262329869_Object_coding_on_the_semantic_graph_for_scene_classification/links/54d24f290cf28e069723d71b/Object-coding-on-the-semantic-graph-for-scene-classification.pdf">[pdf]</a> &nbsp <br><b>Jingjing Chen</b>, Yahong Han, Xiaochun Cao and Qi Tian <br>ACM Multimedia (ACM MM), Barcelona, Spain, October 2013. <br><br></li>
				<li> <b> Visual Saliency Detection Based on Photographic Composition </b> &nbsp <a href="https://www.researchgate.net/profile/Yahong-Han-2/publication/262276134_Visual_saliency_detection_based_on_photographic_composition/links/58d724e592851c44d48d5ebf/Visual-saliency-detection-based-on-photographic-composition.pdf">[pdf]</a> &nbsp <br><b>Jingjing Chen</b>, Handong Zhao, Yahong Han and Xiaochun Cao <br>International Conference on Internet Multimedia Computing and Service (ICIMCS), huangshan, Anhui, China, August 2013. <span style="color:red;">(Best Paper Honorable Mention)</span> <br><br></li>
				<li> <b> Feature selection with Spatial Path Coding for Multimedia Analysis </b> &nbsp <a href="https://www.sciencedirect.com/science/article/pii/S0020025514003843">[pdf]</a> &nbsp <br>Yahong Han, <b>Jingjing Chen</b> and Xiaochun Cao <br>Information Sciences, vol. 281, pp. 523-525, 2014 <br><br></li>
				<li> <b> Object clique representation for scene classification </b> &nbsp <a href="https://ieeexplore.ieee.org/abstract/document/6460754">[pdf]</a> &nbsp <br><b>Jingjing Chen</b>, Xiaochun Cao and Bao Zhang <br>International Conference on Pattern Recognition (ICPR), Tsukuba, Japan, November, 2012. <br><br></li>
				</ul>
		</div>

		<div class="content" id="teach">
			<h5 class="text-primary">Teaching</h5>
			<ul>
			  <li><strong>Instructor</strong>, COMP130124, Computer Vision, Fudan University, For Undergraduate Students, Fall 2021, 2022, 2023, 2024, 2025</li>
			  <li><strong>Instructor</strong>, COMP620028, Information Retrieval, Fudan University, For Graduate Students, Fall 2020, 2021, 2022, 2023, 2024</li>
			  <li><strong>Instructor</strong>, Advanced Media Computing, Fudan University, For Senior Undergraduate and Graduate Students, Summer Semester 2022</li>
			  <li><strong>Instructor</strong>, Frontiers of Computer Vision, For Senior Undergraduate and Graduate Students, Summer Semester 2022</li>
			</ul>
		</div>		

		<div class="content">
			<h5 class="text-primary">Academic Services</h5>
				<ul>
				  <li><strong>Associate Editor</strong>: IEEE TMM, ACM TOMM, Neurocomputing</li>
				  <li><strong>Editor</strong>: ACM SIGMM Records</li>
				  <li><strong>Technical Committee Member</strong>: IEEE-CAS Multimedia Systems & Applications (2021-2025)</li>
				  <li><strong>Conference Area Chair (AC)</strong>: 
				    <ul>
				      <li>ACM Multimedia (2020, 2024)</li>
				      <li>ACL (2023)</li>
				    </ul>
				  </li>
				  <li><strong>Senior Program Chair</strong>: AAAI (2023, 2024, 2025)</li>
				  <li><strong>Conference Organizer</strong>: 
				    <ul>
				      <li>ACM Multimedia 2025 (Demo Co-Chair)</li>
				      <li>Multimedia Modeling 2025 (Program Co-Chair)</li>
				      <li>ACM Multimedia Asia 2025 (Publicity Chair)</li>   
				      <li>ACM Multimedia 2021 (Registration Chair)</li>
				      <li>ACM ICMR 2022 (Publicity Chair)</li>
				      <li>ICME 2022 (Special Session Chair)</li>
				      <li>ACM Multimedia Asia 2023 (Publicity Chair)</li>
				    </ul>
				  </li>
				  <li><strong>Conference Reviewer</strong>: ACM MM, ICLR, CVPR, ICCV, ECCV, etc.</li>
				  <li><strong>Journal Reviewer</strong>: TPAMI, TIP, TKDE, TMM, TOMM, TCSVT, etc.</li>
				</ul>

		</div>


	
		<div class="content" id="team">
		    <h5 class="text-primary" id="team_time">Team Time</h5> 
		    <div class="projects" style="margin-left: 1em; margin-right: 1em">


			<script>
			    // Image list
			    // 2024 dec
			    const imagesBox1 = [
			        { src: "./files/Christmas2024/1.jpg", link: "./files/Christmas2024/1.jpg" },
			        { src: "./files/Christmas2024/2.jpg", link: "./files/Christmas2024/2.jpg" },
			        { src: "./files/Christmas2024/3.jpg", link: "./files/Christmas2024/3.jpg" },
				{ src: "./files/Christmas2024/4.jpg", link: "./files/Christmas2024/4.jpg" }
			    ];
			    let currentIndexBox1 = 0;

			    // 2025 may
			    const imagesBox2 = [
			        { src: "./files/2025may/p1.jpg", link: "./files/2025may/p1.jpg" },
			        { src: "./files/2025may/p2.jpg", link: "./files/2025may/p2.jpg" },
			        { src: "./files/2025may/p3.jpg", link: "./files/2025may/p3.jpg" },
				{ src: "./files/2025may/p4.jpg", link: "./files/2025may/p4.jpg" }
			    ];
			    let currentIndexBox2 = 0;
			
			    // Change image function
			    function changeImage(boxId, direction) {
			        let images, currentIndex;
			
			        // Determine which gallery to modify
			        if (boxId === 1) {
			            images = imagesBox1;
			            currentIndex = currentIndexBox1;
			        }
				if (boxId === 2) {
			            images = imagesBox2;
			            currentIndex = currentIndexBox2;
			        }
			
			        // Update the index
			        currentIndex += direction;
			
			        // Loop around if out of bounds
			        if (currentIndex < 0) {
			            currentIndex = images.length - 1;
			        } else if (currentIndex >= images.length) {
			            currentIndex = 0;
			        }
			
			        // Update the image and link
			        document.getElementById(`gallery-image-${boxId}`).src = images[currentIndex].src;
			        document.getElementById(`gallery-link-${boxId}`).href = images[currentIndex].link;
			
			        // Save the updated index
			        if (boxId === 1) {
			            currentIndexBox1 = currentIndex;
			        }
				if (boxId === 2) {
			            currentIndexBox2 = currentIndex;
			        }
			    }
			</script>

			<!-- box -1 -->
			<div class="projects" style="margin-left: 1em; margin-right: 1em">
			    <div class="boxed" style="border: 1px solid #DDDDDD; padding: 10px; width: auto; height: auto; overflow: hidden;">
			
			        <!-- Gallery Section -->
			        <div class="gallery" 
			            style="position: relative; float: left; margin-left: 10px; margin-right: 10px; border: 1px solid #ccc; width: 320px; height: 260px;">
			
			            <!-- Image with link -->
						
			            <a href="./files/2025sep/25_sep_2.jpg.jpg">
			                <img src="./files/2025sep/25_sep_2.jpg" 
			                     alt="2025 Fall" 
			                     style="width: 100%; height: 100%; object-fit: cover;">
			            </a>
			
			        </div>
			
			        <!-- Text Section -->
			        <h5><strong>New semester!</strong></h5>
			        <p>2025-Sep</p> <br>
			        <p>Today, we celebrate the new members of the Fall 2025 cohort.</p>
			    </div>
			</div>
			<!-- end box -1 -->

			
			<!-- box 0 -->
			<div class="projects" style="margin-left: 1em; margin-right: 1em">
			    <div class="boxed" style="border: 1px solid #DDDDDD; padding: 10px; width: auto; height: auto; overflow: hidden;">
			
			        <!-- Gallery Section -->
			        <div class="gallery" 
			            style="position: relative; float: right; margin-left: 10px; margin-right: 10px; border: 1px solid #ccc; width: 320px; height: 260px;">
			            <button class="prev" 
			                style="position: absolute; left: 0; top: 50%; transform: translateY(-50%); background-color: rgba(0, 0, 0, 0.5); color: white; border: none; padding: 5px; cursor: pointer; z-index: 1;" 
			                onclick="changeImage(2, -1)">&#10094;</button>
			
			            <!-- Image with link -->
			            <a id="gallery-link-2" href="./files/2025may/p1.jpg">
			                <img id="gallery-image-2" src="./files/2025may/p1.jpg" 
			                     alt="Team Christmas2024" 
			                     style="width: 100%; height: 100%; object-fit: cover;">
			            </a>
			
			            <button class="next" 
			                style="position: absolute; right: 0; top: 50%; transform: translateY(-50%); background-color: rgba(0, 0, 0, 0.5); color: white; border: none; padding: 5px; cursor: pointer; z-index: 1;" 
			                onclick="changeImage(2, 1)">&#10095;</button>
			        </div>
			
			        <!-- Text Section -->
			        <h5><strong>Farewell, class of 2025</strong></h5>
			        <p>2025-May</p> <br>
			        <p>Two PhD students and three master's students graduated this year. We spent a precious time with them, having a barbecue and dancing around the bonfire.</p>
			    </div>
			</div>
			<!-- end box 0 -->
			    


			<!-- box 1 -->
			<div class="projects" style="margin-left: 1em; margin-right: 1em">
			    <div class="boxed" style="border: 1px solid #DDDDDD; padding: 10px; width: auto; height: auto; overflow: hidden;">
			
			        <!-- Gallery Section -->
			        <div class="gallery" 
			            style="position: relative; float: left; margin-left: 10px; margin-right: 10px; border: 1px solid #ccc; width: 320px; height: 260px;">
			            <button class="prev" 
			                style="position: absolute; left: 0; top: 50%; transform: translateY(-50%); background-color: rgba(0, 0, 0, 0.5); color: white; border: none; padding: 5px; cursor: pointer; z-index: 1;" 
			                onclick="changeImage(1, -1)">&#10094;</button>
			
			            <!-- Image with link -->
			            <a id="gallery-link-1" href="./files/Christmas2024/1.jpg">
			                <img id="gallery-image-1" src="./files/Christmas2024/1.jpg" 
			                     alt="Team Christmas2024" 
			                     style="width: 100%; height: 100%; object-fit: cover;">
			            </a>
			
			            <button class="next" 
			                style="position: absolute; right: 0; top: 50%; transform: translateY(-50%); background-color: rgba(0, 0, 0, 0.5); color: white; border: none; padding: 5px; cursor: pointer; z-index: 1;" 
			                onclick="changeImage(1, 1)">&#10095;</button>
			        </div>
			
			        <!-- Text Section -->
			        <h5><strong>Merry Christmas 2024!</strong></h5>
			        <p>2024-December</p> <br>
			        <p>We celebrate Christmas at the Anji Resort, enjoying hot springs, gourmet food, karaoke, and skiing. A heartfelt thanks to Yue Yu, Jiayu Wang, Pinye Chen, Xueqiao Wang, Junhao Xu, and Xinghan Li for their hard work in securing the activity funds that made this unforgettable experience possible!</p>
			    </div>
			</div>
			<!-- end box 1 -->

			<!-- box 2-->
			<div class="projects" style="margin-left: 1em; margin-right: 1em">
		        <div class="boxed" style="border: 1px solid #DDDDDD; padding: 10px; width: auto; height: auto; overflow: hidden;">
		            <p>
		                <a href="./files/team_2024_freshman.jpg">
		                    <img src="./files/team_2024_freshman.jpg" 
		                         style="float: right; margin-left: 10px; margin-right: 10px; border: 1px solid #ccc;" 
		                         height="260px" 
		                         alt="Team 2024 Freshman">
		                </a>
		            </p>
		
		            <h5><strong>Welcome new team members!</strong></h5>
		            <p>2024-9-1</p> <br>
		            <p>This year, we welcome new team members: Jiarui Yang, Chao Gong, Yian Li, Xinghan Li, Xueqiao Wang, Jianrong Yan, Tianxiao Xu, Shijie Zhou, and Xiaoyu Chen.</p>
		        </div>
			</div>
		        <!-- end boxed-->


			<!-- box 3-->
			<!-- <div class="projects" style="margin-left: 1em; margin-right: 1em">
		        <div class="boxed" style="border: 1px solid #DDDDDD; padding: 10px; width: auto; height: auto; overflow: hidden;">
		            <p>
		                <a href="./files/team_2024_graduate_1.jpg">
		                    <img src="./files/team_2024_graduate_1.jpg" 
		                         style="float: left; margin-left: 10px; margin-right: 10px; border: 1px solid #ccc;" 
		                         width="380px" 
		                         alt="Farewell to the Class of 2024">
		                </a>
		            </p>
		
		            <h5><strong>Farewell to the Class of 2024!</strong></h5>
		            <p>2024-June</p> <br>
		            <p>This year, we have two doctoral graduates, Zhipeng Wei and Tianwen Qian, along with seven master's graduates: Zixuan Su, Yuehao Yin, Huiyan Qi, Yanqi Wu, Yue Yu, Wenzhuo Xu, and Yiqiang Lv. We wish them all the best in pursuing their dreams~</p>
		        </div>
			</div> -->
			    
		</div>


	


		<div class="content">
			
			<h5 class="text-primary">Group Members</h5>
		
			<strong>PhD Students:</strong>
			<ul>
			  <li>Feng Han (September 2025 – present) </li>
		      <li>Jiayu Wang (September 2025 – present) </li>
			  <li>Jianggang Zhu (September 2025 – present) </li>
			  <li>Zixiang Meng (September 2025 – present) </li>
			  <li>Jiarui Yang (September 2024 – present) <a href="https://flyfaerss.github.io/" target="_blank">homepage</a> </li>
			  <li>Yue Yu (September 2024 – present)</li>
			  <li>Guoshan Liu (September 2024 – present)</li>
			  <li>Yang Jiao (September 2021 – present) <a href="https://sxjyjay.github.io/" target="_blank">homepage</a> </li>
			  <li>Pengkun Jiao (September 2021 – present) <a href="https://pengkun-jiao.github.io/" target="_blank">homepage</a></li>
			  
			</ul>
			
			<strong>Master Students:</strong>
			<ul>
				<li> Fei Li, (September 2025 – present) </li>
				<li> Jiejia Shi, (September 2025 – present) </li>
				<li> Zihao Cai, (September 2025 – present) </li>
				<li> Ziyao Tang, (September 2025 – present) </li>
				<li> Yuhang Zhou, (September 2025 – present) </li>
				<li> Yuyan Chen, (September 2025 – present) </li>
				<li> Chao Gong, (September 2024 – present) </li>
				<li> Yian Li, (September 2024 – present) </li>
				<li> Xinghan Li, (September 2024 – present) </li>
				<li> Xueqiao Wang, (September 2024 – present) </li>
				<li> Jianrong Yan, (September 2024 – present) </li>
				<li> Tianxiao Xu, (September 2024 – present) </li>
				<li> Shijie Zhou, (September 2024 – present) </li>
				<li> Xiaoyu Chen, (September 2024 – present) </li>
				<li> Ziyi Gao, (September 2023 – present) </li>
				<li> Junhao Xu, (September 2023 – present) </li>
				<li> Pinye Chen, (September 2023 – present) </li>
				<li> Feng Han, (September 2023 – present) </li>
				<li> Jiayu Wang, (September 2023 – present) </li>
				<li> Xinlan Wu, (September 2023 – present) </li>
			</ul>
				

		</div>

		<div class="content">
			<h5 class="text-primary">Former Members</h5>
				<ul>
					<li> <a href="https://scholar.google.com.hk/citations?user=M6CSZVsAAAAJ&hl=zh-CN" target="_blank">Xue Song</a> (Ph.D. student): September 2021 - July 2025, now at <a>Huawei (Singapore)</a>  <br> - <i> Thesis title: Controllable Visual Content Generation & Editing </i> 
					<li> <a href="https://scholar.google.com/citations?user=y01ICTAAAAAJ&hl=zh-CN" target="_blank">Kai Chen</a> (Ph.D. student): September 2020 - July 2025, now at <a>BiteDance</a>  <br> - <i> Thesis title: Adversarial Robustness in Video Recognition Models </i> 
					<li> <a href="https://zhipeng-wei.github.io" target="_blank">Zhipeng Wei</a> (Ph.D. student): September 2021 - June 2024, now at <a>UC Berkeley</a> as Postdoc researcher <br> - <i> Thesis title: Adversarial Robustness Evaluation for Deep Visual Models </i> 
					<li> <a href="https://qiantianwen.github.io" target="_blank">Tianwen Qian</a> (Ph.D. student): September 2019 - June 2024, now at <a href="https://www.ecnu.edu.cn/" target="_blank">ECNU</a> as Pre-tenured Associate Professor <br> - <i> Thesis title: Multi-modal Visual Question Answering </i> 
					<li> <a href="https://ccds.fzu.edu.cn/info/1204/10297.htm" target="_blank">Linhai Zhuo</a> (Ph.D. student): September 2019 - June 2023, now at <a>Fuzhou University</a> as Lecturer <br> - <i> Thesis title: Few-shot Image Recognition with Knowledge Transfer </i>  
					<li> Yinxuan Gui (Master Student), September 2022 – July 2025, Now at <a>Singapore Management University for PhD degree </a>
				    <li> Haoxiang Chen (Master Student), September 2022 – Jan 2026 </li>
				    <li> Jiacheng Zhang (Master Student), September 2022 – July 2025, Now at <a>Hetao for PhD Degree</a>
				    <li> Wentao Tian (Master Student), September 2022 – July 2025, Now at <a>Shopee</a>
					<li> Zixuan Su (Master Student): September 2021 - January 2024, Now at <a>Tencent</a>
					<li> Yuehao Yin (Master Student): September 2021 - January 2024 
					<li> Huiyan Qi (Master Student): September 2021 - January 2024, now at <a>SMU</a> as Research Associate
					<li> Yanqi Wu (Master Student): September 2021 - June 2024, Now at <a>Papal</a>. 
					<li> Yue Yu (Master Student): September 2021 - June 2024, now pursuing PhD degree at Fudan
					<li> WenZhuo Xu (Master Student): September 2021 - June 2024, now at <a>360</a>
					<li> Yiqiang Lv (Master Student): September 2021 - June 2024, now at <a>Pinduoduo</a>
					<li> Jianggang Zhu (Master Student): September 2020 - January 2023 
					<li> Fan Luo(Master Student): September 2020 - January 2023, now at <a>Century Frontier Asset Management</a>
					<li> Xueqing Zhou (Master Student): September 2019 - June 2024, now at <a>Antgroup </a>
					<li> Jixiang Gao (Master Student): September 2019 - January 2022, now at <a>Antgroup</a>
					<li> Jianlong Wu(Master Student): September 2019 - January 2022, now <a>running a startup company</a> 
					<li> Jinmian Cai(Master Student): September 2019 - January 2022,now at <a>Tencent</a> 
					<li> Tao Sha (Master Student): September 2019 - January 2022, now at <a>Ctrip</a>

				</ul>
		</div>
			
		<div class="content">
			<h5 class="text-primary">Miscellaneous</h5>


			<p>When I have spare time, I enjoy cooking. </p>

			<p>
			</p>
		</div>
		Webpage template borrowed from Prof. <a href="https://cocoxu.github.io/#advise">Wei Xu</a>.

	
</body></html>
